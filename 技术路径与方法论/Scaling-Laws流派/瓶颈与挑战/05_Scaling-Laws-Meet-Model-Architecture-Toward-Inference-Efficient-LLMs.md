# Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs

**类型**: 论文 (Paper)

**链接**: [https://arxiv.org/abs/2510.18245](https://arxiv.org/abs/2510.18245)

## 摘要

Introduces a conditional scaling law that augments the Chinchilla framework with architectural information to address the trade-off between model accuracy and inference efficiency, showing that optimized architectures can achieve higher throughput.

## 下载

请访问上述链接查看完整论文。
