# Scaling Laws for Neural Language Models

**类型**: 论文 (Paper)

**链接**: [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)

## 摘要

The foundational paper by Kaplan et al. establishing the empirical power-law relationships between language model performance (loss) and model size, dataset size, and compute. It provides the theoretical basis for the 'Scaling-Laws School' and the strategy of continuous expansion.

## 下载

请访问上述链接查看完整论文。
