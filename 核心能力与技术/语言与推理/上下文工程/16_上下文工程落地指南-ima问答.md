# 我问
从生产环境落地的视角，请系统梳理一下"上下文工程"在工程化过程中遇到的主要挑战。要求：
- 按模块分组（模型层/记忆系统/多Agent&工具/评估监控等）
- 对每个挑战说明：问题 → 典型影响 → 常见场景
- 聚焦 10 个以内的核心挑战，而不是穷举所有可能问题
- 给出优先级判断（哪些是绝大多数团队都会遇到的）

# ima答

**核心理解**：上下文工程的本质是在**有限的上下文窗口内做信息调度和记忆管理**。对工程落地而言，最重要的三个维度是：**① 上下文长度与成本控制 ② 记忆系统设计 ③ 工具/多Agent编排**。

下面按模块梳理核心挑战，每个挑战都标注了**优先级**（P0=必须解决，P1=中等优先级，P2=高级优化）。

---

## 一、模型层面：上下文窗口与计算效率（P0）

### 挑战 1：有限上下文窗口 + 成本爆炸
**问题**：
- Transformer 模型上下文窗口有硬限制（4k-128k token不等）
- 处理长上下文时计算复杂度呈二次增长，成本和延迟急剧上升
- 超出训练长度时会出现"中间丢失"现象（模型忽略中间部分信息）

**典型影响**：
- 多轮对话超过窗口限制后，模型"失忆"，重复问题或丢失关键上下文
- 长文档分析（如法律合同、技术文档）需要拆分处理，增加工程复杂度
- Token成本直接影响商业可行性（GPT-4每百万token成本可达数十美元）

**常见场景**：
- 客服Bot处理复杂多轮对话（10+轮后开始"忘记"用户需求）
- 代码助手需要理解整个代码库上下文
- 文档问答系统需要跨章节理解

**工程师视角**：
> 这是最基础但最容易被低估的问题。很多团队一开始设计时没考虑上下文预算，等上线后发现成本是预期的5-10倍。**建议：第一周就要建立token统计监控，设定单次调用上限。**

---

## 二、记忆系统：存储与检索瓶颈（P0）

### 挑战 2：长期记忆维护困难
**问题**：
- LLM本身无状态，跨会话后会完全"失忆"
- 需要构建外部记忆系统（短期/长期分层），但实现复杂度高
- 记忆压缩/摘要策略可能丢失关键细节或引入幻觉

**典型影响**：
- 用户每次对话都要重新解释背景（体验极差）
- 系统无法累积学习用户偏好
- 压缩不当会导致关键信息丢失（如"上次说要蓝色"被压缩成"有颜色偏好"）

**常见场景**：
- 个人助理型应用（需要记住用户习惯、历史决策）
- 企业知识库问答（需要记住用户的角色、权限、查询历史）
- 项目管理Agent（需要跨会话跟踪任务状态）

### 挑战 3：检索精度与延迟的权衡
**问题**：
- 向量检索召回率不足，关键信息可能被漏掉
- 混合检索（向量+关键词+图谱）能提升效果，但延迟增加
- 检索结果需要重排序，又增加一次LLM调用

**典型影响**：
- RAG系统给出无关答案（"检索到了，但没检索对"）
- 为了保证召回，返回过多chunk，导致上下文窗口被撑爆
- 延迟从300ms增加到2-3秒，用户体验下降

**工程师视角**：
> **记忆系统是区分"玩具demo"和"生产系统"的分水岭**。建议起步时用简单方案：最近N轮对话 + Top-k向量检索，但要预留扩展接口。千万别一上来就想实现类人类的完美记忆系统。

---

## 三、多Agent与工具：协调与编排（P1）

### 挑战 4：工具爆炸与提示膨胀
**问题**：
- 系统集成的工具越来越多（10+个工具很常见），每个工具的描述都要占用上下文
- 提示中工具描述过多，导致：① token成本激增 ② 模型选错工具 ③ 虚构不存在的工具

**典型影响**：
- 单次调用基础成本从100 token增加到2000+ token（光工具描述就占这么多）
- 模型在20个工具中选择时准确率显著下降
- 频繁出现"调用了不该调用的工具"

**常见场景**：
- 企业内部智能助手（需要集成CRM、ERP、文档系统等数十个API）
- 开发者工具（需要集成Git、测试框架、部署工具等）

**解决思路预告**：
- RAG-MCP：根据查询动态选择工具，而不是把所有工具都塞进提示
- 工具分组：按场景/模块分组，只加载相关组

### 挑战 5：多Agent协调复杂度
**问题**：
- 子Agent之间缺乏统一上下文，导致决策冲突或重复劳动
- 通信开销大，且容易因某个Agent幻觉而导致整个系统偏离目标
- 依赖关系不透明，难以调试和验证

**典型影响**：
- Agent A已经检索过文档，Agent B又重复检索
- Agent A认为任务完成，Agent B还在等待输入（死锁）
- 某个Agent产生幻觉，其他Agent基于错误信息继续执行

**常见场景**：
- 复杂工作流（如"市场调研 → 生成报告 → 数据可视化"需要多个专业Agent）
- 并行任务处理（如同时爬取多个网页并汇总）

**工程师视角**：
> 多Agent不是万能药，**单Agent能解决的问题不要上多Agent**。多Agent的引入会让调试难度指数级上升。建议先做好单Agent的工具链，再考虑拆分。

---

## 四、上下文管理：污染与冲突（P0）

### 挑战 6：上下文"四大失败模式"
**问题**：
- **污染**：错误信息（幻觉、工具异常输出）进入上下文后被反复引用
- **干扰**：冗余信息过多，稀释模型注意力，输出质量下降
- **混乱**：无关内容（如过多系统提示）引发不必要操作
- **冲突**：上下文中存在矛盾信息（如缓存的旧数据 vs 新查询结果），模型无法取舍

**典型影响**：
- 一次工具调用出错后，模型陷入"错误循环"（反复基于错误信息做决策）
- 对话历史过长后，模型开始"胡言乱语"（被无关信息干扰）
- 用户修改需求，但模型还在执行旧需求（冲突未解决）

**常见场景**：
- 长任务执行中途某个环节出错
- 用户在对话中途推翻之前的要求
- 系统缓存的数据与实时数据不一致

**工程师视角**：
> 这是最容易被忽视但最致命的问题。**强烈建议**：
> - 给每个工具调用加错误处理和验证
> - 在上下文中明确区分"已确认信息"和"待验证信息"
> - 定期做上下文"快照"，出错时能回滚

---

## 五、生产环境：成本、性能、安全（P0）

### 挑战 7：成本控制难题
**问题**：
- 长上下文、频繁检索、多轮对话都会快速消耗token
- 商业部署中，token成本可能占到总成本的30-50%
- 缺乏细粒度的成本归因（不知道成本主要花在哪个环节）

**典型影响**：
- 预算按100次/天设计，实际用户用了1000次，成本超支10倍
- 某个"无限循环"bug导致单个会话消耗数十万token
- 无法按用户/场景做精细化成本控制

**工程师视角**：
> **第一天就要建立成本监控**：
> - 每个用户/会话的token消耗
> - 每个工具调用的成本
> - 设置告警阈值（单次>10k token、单日>100万token等）

### 挑战 8：高并发下的延迟与稳定性
**问题**：
- 上下文处理管道（检索→排序→压缩→调用LLM）串行执行，延迟累加
- 高并发时，向量数据库、LLM API都可能成为瓶颈
- 缺乏熔断、降级机制

**典型影响**：
- 用户量从100增长到1000时，平均延迟从1秒增加到5秒
- 某个环节超时，整个请求失败，没有部分降级
- 高峰期系统完全不可用

### 挑战 9：数据安全与隐私
**问题**：
- 上下文中可能包含敏感数据（用户信息、业务机密）
- 多租户场景下，需要严格隔离不同用户的上下文
- LLM提供商可能记录请求日志（数据泄露风险）

**典型影响**：
- 用户A的上下文泄露给用户B（灾难性事故）
- 敏感数据通过日志/监控系统泄露
- 违反GDPR、等保等合规要求

**工程师视角**：
> 安全不是"加固"，而是"设计"阶段就要考虑：
> - 上下文按用户/租户严格隔离（数据库、缓存、日志都要隔离）
> - 敏感字段脱敏后再输入LLM
> - 使用私有化部署或签署严格的数据保护协议

---

## 六、评估与调试：可观测性不足（P1）

### 挑战 10：缺乏有效评估与调试手段
**问题**：
- 传统NLP指标（BLEU、Rouge）不适用于上下文工程
- 上下文管道涉及多个动态组件（检索、压缩、工具调用），故障定位困难
- 缺乏"记忆一致性""推理质量""工具选择准确率"等专项指标

**典型影响**：
- 系统上线后发现效果不好，但不知道是哪个环节出了问题
- 改了检索策略，不知道对整体效果有多少提升
- 用户反馈"有时好有时坏"，无法复现问题

**工程师视角**：
> 建议从第一天就接入完整的trace系统（如LangSmith、LangFuse）：
> - 记录每次请求的完整上下文、中间结果、最终输出
> - 按场景建立评估集（10-50个典型case）
> - 每次改动都要跑回归测试

---

## 优先级总结（给工程师的行动建议）

| 优先级 | 挑战领域 | 第一阶段必须做 | 第二阶段优化 |
|--------|---------|----------------|-------------|
| **P0** | 上下文窗口 | Token统计+限额 | 引入压缩/摘要 |
| **P0** | 记忆系统 | 最近N轮+简单RAG | 分层记忆+图检索 |
| **P0** | 上下文管理 | 错误处理+验证 | 快照+回滚机制 |
| **P0** | 成本控制 | 监控+告警 | 按场景优化策略 |
| **P0** | 安全隔离 | 租户隔离+脱敏 | 细粒度权限控制 |
| **P1** | 工具管理 | 工具数量<10 | RAG-MCP动态加载 |
| **P1** | 多Agent | 能不拆就不拆 | 引入编排框架 |
| **P1** | 评估调试 | 基础日志+trace | 专项指标+回归测试 |
| **P2** | 多模态 | 暂不考虑 | 按需引入 |

**一句话总结**：上下文工程不是"提示词优化"，而是一个需要系统化设计的工程领域。绝大多数问题不是靠"更好的模型"解决，而是靠**更好的架构设计和工程纪律**。

# 我问
针对这些挑战，请给出一份**工程落地指南**。要求：
- 按"起步期（1-2周）/ 成长期（1-3个月）/ 成熟期（持续优化）"三阶段给出方案
- 每个阶段列出3-5条最关键的解决思路与技术方案，并说明为什么优先做这些
- 概念解释要接地气，尽量给实际场景示例
- 给出一个"第一个月落地checklist"，让工程师能直接照着做

# ima答

**核心思想**：从"被动填充上下文"转向"主动工程化管理上下文"。把上下文管理想象成**操作系统的内存管理**：在有限资源下，智能调度、压缩和呈现最关键的信息。

上下文工程有四大核心策略：**写入（Write）/ 选择（Select）/ 压缩（Compress）/ 隔离（Isolate）**。下面按照实际落地的三个阶段，给出可执行的技术方案。

---

## 📍 落地路线图总览

```
阶段一：起步期（第1-2周）
目标：搭出最小可用系统，验证核心流程
重点：基础上下文管理 + 简单RAG + 成本监控

阶段二：成长期（第1-3个月）
目标：支持生产流量，优化成本和体验
重点：分层记忆 + 工具链优化 + 多Agent协作

阶段三：成熟期（持续优化）
目标：性能和成本达到最优平衡
重点：高级压缩 + 长上下文架构 + 完整可观测性
```

## 阶段一：起步期（1-2周，搭建MVP）

### 目标
以**最低复杂度**搭出一个可工作的上下文管线，验证技术可行性和业务价值。

### 核心方案

#### 1. 基础上下文管理：滑动窗口 + 轻量摘要

**做法**：
- 只保留最近 N 轮对话（N=3-5，根据模型上下文限制调整）
- 每 5 轮对话用 LLM 生成一个简短摘要（100-200 token）
- 结构：`[系统提示] + [历史摘要] + [最近N轮] + [用户当前输入]`

**示例代码逻辑**：
```python
def build_context(history, max_turns=5):
    recent = history[-max_turns:]  # 最近5轮
    if len(history) > max_turns:
        # 对早期历史生成摘要
        summary = summarize(history[:-max_turns])
    else:
        summary = ""
    
    return f"{SYSTEM_PROMPT}\n\n{summary}\n\n{format_turns(recent)}"
```

**为什么优先做**：
- 实现简单，1-2天可完成
- 能覆盖80%的短对话场景
- 成本可控（摘要开销低）

**典型场景**：客服Bot处理5轮以内的简单咨询

---

#### 2. 简单RAG：文档检索 + Top-k召回

**做法**：
- 把知识库文档切分成 chunk（500-1000 token/chunk）
- 用 OpenAI Embedding / 通义向量服务生成向量
- 存入向量数据库（Pinecone / Milvus / 本地Faiss）
- 查询时检索 Top-3 最相关 chunk，直接拼接到上下文

**为什么优先做**：
- RAG 是解决"知识边界"问题的最直接方式
- 向量检索成熟度高，开箱即用
- 可以快速验证"检索+生成"的效果

**典型场景**：企业文档问答、产品手册查询

**注意事项**：
- chunk 大小影响召回质量，建议先用 512-1024 token，后续根据效果调整
- 检索数量（Top-k）需要权衡：太少漏信息，太多占用上下文

---

#### 3. Token监控与限额（最容易被忽视但最重要）

**做法**：
- 每次 LLM 调用记录：输入 token / 输出 token / 总成本
- 按用户/会话/接口维度统计
- 设置单次调用上限（如 8k token）和每日告警阈值

**示例监控指标**：
```
- 平均每次调用 token 数
- P95 / P99 token 数
- 单用户日消耗 token
- 异常请求告警（单次 > 10k token）
```

**为什么优先做**：
- 上线后才发现成本问题，为时已晚
- 很多 bug（如死循环）会通过异常 token 消耗暴露出来

---

#### 4. 清晰的系统Prompt + 工具调用约束

**做法**：
- 写一个结构化的系统 Prompt，包含：
  - 角色定义（你是什么）
  - 任务目标（要做什么）
  - 行为约束（不要做什么）
  - 输出格式要求
- 如果涉及工具调用，明确"何时调用""如何调用""失败如何处理"

**示例**：
```
你是企业知识库助手。
目标：根据文档回答用户问题，不要编造信息。
约束：
- 如果文档中没有答案，明确告知"文档中未找到相关信息"
- 每次回答需引用来源文档
- 不要调用超过3个工具
```

**为什么优先做**：
- 系统 Prompt 是整个系统的"宪法"，决定了行为边界
- 明确约束能显著减少幻觉和错误工具调用

---

### 起步期总结：第1-2周Checklist

- [ ] **Day 1-2**：搭建基础调用链（Prompt → LLM → 解析输出）
- [ ] **Day 3-4**：实现滑动窗口对话历史管理
- [ ] **Day 5-7**：接入向量数据库，实现简单 RAG
- [ ] **Day 8-10**：加入 Token 监控和成本统计
- [ ] **Day 11-14**：端到端测试，收集 10-20 个典型 case 做评估

**预期效果**：能跑通基本流程，回答简单问题，成本可控。

---

## 阶段二：成长期（1-3个月，进入生产）

### 目标
支持真实用户流量，优化成本和用户体验，处理更复杂的场景。

### 核心方案

#### 1. 分层记忆架构：短期+长期记忆

**做法**：
- **短期记忆**（上下文窗口）：当前对话的最近几轮 + 任务状态
- **长期记忆**（外部存储）：
  - 用户画像（偏好、历史决策）存入KV数据库
  - 历史对话片段向量化存入向量数据库
  - 关键实体和关系存入图数据库（可选）

**检索策略**：
- 每次对话开始时，从长期记忆中检索相关历史（如"用户上次问过类似问题"）
- 对话结束时，提取关键信息更新长期记忆

**参考架构**（类 MemGPT）：
```
用户输入 
  ↓
检索长期记忆（Top-3历史对话）
  ↓
构建短期上下文 = [系统Prompt] + [长期记忆] + [最近对话]
  ↓
LLM处理
  ↓
更新长期记忆（如果有新的用户偏好）
```

**典型场景**：
- 个人助理（记住用户的日程安排、工作习惯）
- 项目管理 Agent（跨会话跟踪任务进度）

**为什么这个阶段做**：
- 起步期积累了足够数据，能验证长期记忆的价值
- 用户开始有"它应该记住我"的期待

---

#### 2. RAG 优化：混合检索 + 重排序

**做法**：
- **混合检索**：向量检索（语义相似） + BM25（关键词匹配）
- **重排序**：用小模型（如 BGE-reranker）对召回结果重新打分
- **chunk 优化**：
  - 大 chunk（1024 token）用于检索
  - 小 chunk（256 token）用于实际输入（减少噪音）

**效果提升**：
- 召回率提升 20-30%
- 准确率提升 15-20%

**成本增加**：
- 重排序增加约 50ms 延迟，但 token 成本降低（更少无关内容）

---

#### 3. 工具链管理：动态加载 + 错误处理

**做法**：
- **工具分组**：把工具按场景分组（如"文档类""数据库类""API类"）
- **动态加载**（RAG-MCP思路）：
  - 不把所有工具描述都塞进 Prompt
  - 根据用户查询先判断场景，再加载对应组的工具
- **错误处理**：
  - 每个工具调用包裹 try-catch
  - 失败时给模型明确的错误信息（而不是让它自己猜）
  - 限制重试次数（避免无限循环）

**示例**：
```python
def call_tool(tool_name, params):
    try:
        result = execute_tool(tool_name, params)
        return {"success": True, "result": result}
    except Exception as e:
        return {
            "success": False, 
            "error": f"工具{tool_name}调用失败: {str(e)}",
            "suggestion": "请检查参数或尝试其他方法"
        }
```

**为什么这个阶段做**：
- 工具数量开始增长（>5个），静态加载开始出现问题
- 生产环境开始遇到各种工具调用异常

---

#### 4. 多Agent协作（按需引入）

**什么时候需要多Agent**：
- 任务有明确的"规划→执行→汇总"流程
- 不同子任务需要不同的专业能力
- 单Agent上下文已经管理不过来（>20k token）

**起手式架构**：
- **主Agent（Planner）**：负责任务分解和结果汇总
- **子Agent（Worker）**：各自有独立上下文，完成专项任务
- **通信机制**：通过结构化消息传递（JSON）

**示例场景**：
```
用户：帮我做一份竞品分析报告

主Agent拆解：
1. 网页爬取Agent：抓取竞品官网信息
2. 数据分析Agent：对比产品功能和价格
3. 报告生成Agent：汇总成文档

每个子Agent有独立上下文，互不干扰
```

**坑点提醒**：
- 不要为了多Agent而多Agent，单Agent能解决别拆
- 多Agent调试难度大，建议先做好日志和trace
- 通信协议要规范，避免"鸡同鸭讲"

**参考框架**：AgentScope / OpenAI Swarm / AutoGen

---

#### 5. 上下文压缩：智能摘要

**做法**：
- 对过长的对话历史，不是简单截断，而是用LLM生成摘要
- 摘要要保留：关键事实、用户意图、已做决策
- 摘要占用上下文的20-30%

**示例Prompt**：
```
请总结以下对话的关键信息：
- 用户的核心需求是什么
- 已经确认的事实和决策
- 待解决的问题

对话历史：
[...]
```

**参考系统**：Claude Code 的自动压缩机制

---

### 成长期总结：第1-3个月Checklist

- [ ] **Week 1-2**：实现分层记忆架构（短期+长期）
- [ ] **Week 3-4**：优化RAG（混合检索+重排序）
- [ ] **Week 5-6**：建立工具链管理和错误处理机制
- [ ] **Week 7-8**：评估是否需要多Agent，若需要则实现主-子Agent架构
- [ ] **Week 9-12**：加入上下文压缩和摘要，持续优化成本

**预期效果**：稳定支持生产流量，用户满意度提升，成本下降30-50%。

---

## 阶段三：成熟期（持续优化）

### 目标
针对高频痛点做深度优化，实现成本、性能、效果的最佳平衡。

### 核心方案

#### 1. 长上下文架构优化

**方案选择**：
- **方案A：更长上下文模型**
  - 使用支持128k+的模型（如Claude 3/GPT-4 Turbo）
  - 适合：对话轮数多但QPS不高的场景
  - 成本：贵，但减少了压缩和检索的工程复杂度

- **方案B：状态空间模型**（前沿）
  - Mamba 等线性复杂度架构
  - 适合：对成本极度敏感的场景
  - 风险：生态不成熟，需要自己训练/微调

- **方案C：外部缓存机制**
  - 用 Prompt Cache（如 Anthropic 的 Prompt Caching）
  - 对不变的部分（如系统Prompt、文档）做缓存
  - 成本降低 50-90%

**建议**：优先尝试方案C（Prompt Cache），投入产出比最高。

---

#### 2. 高级压缩技术

**方案**：
- **Recursive Context Compression (RCC)**：多轮递归压缩
- **Pruning（修剪）**：用模型判断哪些句子不相关，直接删除
- **结构化压缩**：把自然语言压缩成JSON/表格等结构化格式

**典型场景**：
- 法律文档分析（几十页合同压缩成关键条款）
- 代码库理解（整个仓库压缩成模块依赖图）

**参考系统**：Provence（将压缩视为序列标注任务）

---

#### 3. 图增强 RAG（GraphRAG）

**什么时候需要**：
- 知识高度结构化（如企业组织架构、产品依赖关系）
- 需要多跳推理（如"找出A公司投资的B公司的CEO"）

**做法**：
- 把文档转成知识图谱（实体+关系）
- 查询时不仅检索文本，还检索相关子图
- 把子图线性化后输入LLM

**效果**：
- 对复杂查询的准确率提升50%+
- 但实施成本高，需要专业团队

---

#### 4. 完整的可观测性体系

**必备工具**：
- **Trace系统**：LangSmith / LangFuse / 自建
- **监控面板**：Grafana / 自建
- **评估框架**：
  - 任务成功率（端到端）
  - 工具调用准确率
  - 记忆召回率
  - 成本和延迟分布

**专项指标**：
```
- 上下文利用率（实际使用 / 窗口大小）
- 压缩比（压缩后 / 压缩前）
- 检索命中率（相关文档 / 召回文档）
- 多Agent协调成功率
```

**为什么这个阶段做**：
- 前期快速迭代，来不及建完整体系
- 到这个阶段，边际收益需要靠精细化运营

---

#### 5. 工程最佳实践

**文件系统作为外部上下文**：
- 将大型中间结果（网页内容、文档草稿）写入文件
- 上下文中只保留文件路径或摘要
- 需要时按需读取，极大节省 Token

**实时引导与状态保持**：
- 不断将任务目标重写到上下文末尾，保持模型注意力
- 保留失败尝试和错误信息，让模型从中学习

**参考系统**：Claude Code、Manus

---

### 成熟期总结

这个阶段没有固定timeline，而是根据业务痛点持续优化：
- 成本是瓶颈 → 优先做压缩和缓存
- 准确率是瓶颈 → 优先做GraphRAG和重排序
- 延迟是瓶颈 → 优先做并行化和缓存

---

## 🎯 第一个月落地Checklist（拿来即用）

### Week 1：基础设施
- [ ] 选定主力LLM（GPT-4/Claude/国产模型）
- [ ] 搭建调用链和基础日志
- [ ] 实现滑动窗口对话管理（最近5轮）
- [ ] 建立Token监控

### Week 2：RAG接入
- [ ] 文档切分和向量化
- [ ] 选择向量数据库（Pinecone/Milvus/Faiss）
- [ ] 实现Top-k检索
- [ ] 测试10个典型问题的召回效果

### Week 3：工具和错误处理
- [ ] 接入2-3个核心工具（如搜索、数据库查询）
- [ ] 实现工具调用的错误处理
- [ ] 限制重试次数和Token上限
- [ ] 测试工具调用的边界case

### Week 4：评估和优化
- [ ] 建立评估集（20-50个case）
- [ ] 跑一轮端到端测试
- [ ] 分析成本分布（哪个环节花费最多）
- [ ] 根据数据调整策略（检索数量、摘要频率等）

**预期产出**：
- 一个能跑通的MVP系统
- 20-50个测试case的效果数据
- 成本和延迟的baseline数据
- 明确的下一步优化方向

---

## 💡 工程师最容易踩的5个坑

1. **过早优化**：一上来就想实现完美的记忆系统，结果3个月还没上线
   - **建议**：先上简单方案，根据数据迭代

2. **忽视成本监控**：等上线后发现成本超支10倍
   - **建议**：第一天就要建立Token统计

3. **盲目引入多Agent**：为了"架构先进"拆分Agent，结果调试噩梦
   - **建议**：单Agent能解决就不要拆

4. **过度依赖长上下文**：觉得有128k窗口就能"无脑塞"
   - **建议**：长上下文成本高且效果未必好，该压缩还得压缩

5. **缺少错误处理**：工具调用失败后系统陷入死循环
   - **建议**：每个工具调用都要有超时和重试限制

---

## 📚 参考架构和系统

| 系统/方案 | 核心亮点 | 适合场景 |
|----------|---------|----------|
| **MemGPT** | LLM as OS，明确的内存管理（页换入/换出） | 需要长期记忆的Agent |
| **Claude Code** | 文件系统作为上下文，子Agent隔离 | 代码助手、复杂工作流 |
| **RAG-MCP** | 动态工具选择，避免提示膨胀 | 工具数量>10的系统 |
| **GraphRAG** | 图结构增强检索 | 知识高度结构化的场景 |
| **Provence** | 上下文修剪（序列标注） | 需要极致压缩的场景 |
| **AgentScope/Swarm** | 多Agent编排框架 | 复杂多Agent协作 |

---

## 最后：一句话总结

**上下文工程的成功不是靠"更好的模型"，而是靠"更好的架构设计和工程纪律"。** 

从简单方案开始，根据真实数据迭代，永远监控成本和效果，这才是工程师的正确姿势。