# Research

**来源**: [https://www.anthropic.com/research](https://www.anthropic.com/research)

---

# Research \ Anthropic

原文链接: https://www.anthropic.com/research

# Research

Our research teams investigate the safety, inner workings, and societal impacts of AI models – so that artificial intelligence has a positive impact as it becomes increasingly capable.

Research teams:[Interpretability](/research/team/interpretability)[Alignment](/research/team/alignment)[Societal Impacts](/research/team/societal-impacts)

### Interpretability

The mission of the Interpretability team is to discover and understand how large language models work internally, as a foundation for AI safety and positive outcomes.

### Alignment

The Alignment team works to understand the risks of AI models and develop ways to ensure that future ones remain helpful, honest, and harmless.

### Societal Impacts

Working closely with the Anthropic Policy and Safeguards teams, Societal Impacts is a technical research team that explores how AI is used in the real world.

### Frontier Red Team

The Frontier Red Team analyzes the implications of frontier AI models for cybersecurity, biosecurity, and autonomous systems.

![Video thumbnail](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/9c6c74e02c970b12846860817dffde9a.jpg)

[## Project Fetch: Can Claude train a robot dog?

PolicyNov 12, 2025

How much does Claude help people program robots? To find out, two teams of Anthropic staff raced to teach quadruped robots to fetch beach balls. The AI-assisted team completed tasks faster and was the only group to make real progress toward full autonomy.](/research/project-fetch-robot-dog)

[InterpretabilityOct 29, 2025

#### Signs of introspection in large language models

Can Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models.](/research/introspection)[InterpretabilityMar 27, 2025

#### Tracing the thoughts of a large language model

Circuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.](/research/tracing-thoughts-language-model)[AlignmentFeb 3, 2025

#### Constitutional Classifiers: Defending against universal jailbreaks

These classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered.](/research/constitutional-classifiers)[AlignmentDec 18, 2024

#### Alignment faking in large language models

This paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.](/research/alignment-faking)

## Publications

Search

DateCategoryTitle

* [Nov 25, 2025Economic Research

  Estimating AI productivity gains from Claude conversations](/research/estimating-productivity-gains)
* [Nov 24, 2025Product

  Mitigating the risk of prompt injections in browser use](/research/prompt-injection-defenses)
* [Nov 21, 2025Alignment

  From shortcuts to sabotage: natural emergent misalignment from reward hacking](/research/emergent-misalignment-reward-hacking)
* [Nov 12, 2025Policy

  Project Fetch: Can Claude train a robot dog?](/research/project-fetch-robot-dog)
* [Nov 4, 2025Alignment

  Commitments on model deprecation and preservation](/research/deprecation-commitments)
* [Oct 29, 2025Interpretability

  Signs of introspection in large language models](/research/introspection)
* [Oct 14, 2025Policy

  Preparing for AI’s economic impact: exploring policy responses](/research/economic-policy-responses)
* [Oct 9, 2025Alignment

  A small number of samples can poison LLMs of any size](/research/small-samples-poison)
* [Oct 6, 2025Alignment

  Petri: An open-source auditing tool to accelerate AI safety research](/research/petri-open-source-auditing)
* [Oct 3, 2025Policy

  Building AI for cyber defenders](/research/building-ai-cyber-defenders)

[See more](#)

![Estimating AI productivity gains from Claude conversations](https://www-cdn.anthropic.com/images/4zrzovbb/website/1c3d1af62032009538b8bf5864139ca124b06741-1000x1000.svg)

Join the Research team

[See open roles](/jobs)

---

*爬取时间: 2025-11-28 21:56:30*
