# Stanford AI Safety

**来源**: [https://aisafety.stanford.edu/](https://aisafety.stanford.edu/)

---

# Stanford AI Safety

原文链接: https://aisafety.stanford.edu/

[Stanford University](https://stanford.edu)

![](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/b4be05a562674f63921be0ff721bc9f0.png)

Stanford Center for AI Safety



![](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/19fa6c76eaeb503c09579c5d2b54de1f.png)  
  
  
![](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/ef0c71e769237fc24b143b8f95be3039.png)

#### The mission of the Stanford Center for AI Safety is to develop rigorous techniques for building safe and trustworthy AI systems and establishing confidence in their behavior and robustness, thereby facilitating their successful adoption in society.

[## **Read more in the white paper**](whitepaper.pdf)

FLAGSHIP PROJECTS

[### Responsible AI Assessment

We aim to develop a Responsible AI (RAI) assessment framework consisting of evaluation metrics and mitigation strategies.
This research aims to combine industry and academic perspectives to create a comprehensive guide for building RAI.
... (click for more)](#)
  
[### Error Pocket Retrieval in Computer Vision from Unlabeled Data

AI practitioners often need to perform model comparison to select the most suitable models for integration into larger pipelines,
or to cross-check fine-tuned models against their predecessors for desired improvements. ... (click for more)](#)

  
  
  
  


[### Verifying Learning-Based Controllers using Neural Lyapunov Barrier Certificates

Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating agents that control autonomous systems.
However, the black-box nature of DRL agents limits their deployment in real-world safety-critical applications. A promising approach ... (click for more)](#)
  
[### Safe Navigation with Learned Environment Maps

Recently, 3D neural maps learned from sensor data have shown astounding results
in visual realism and detail, as well as more compactness and efficiency compared to traditional
map representations. However, despite their strengths over traditional map representations,
neural maps are still limited by ...gi (click for more)](#)

×
  

We aim to develop a Responsible AI (RAI) assessment framework consisting of evaluation metrics and mitigation strategies.
This research aims to combine industry and academic perspectives to create a comprehensive guide for building RAI.
As AI technology advances, governments are introducing requirements for transparency, accountability, and data protection.
Compliance with these regulations is essential to avoid substantial fines and penalties.
The absence of clear metrics and key performance indicators (KPIs) poses a challenge to assessing RAI,
but companies can develop and apply relevant metrics within a robust framework.
Regular assessments offer valuable insights into AI systems' current states, highlighting areas of excellence and needed improvements,
allowing for targeted actions to enhance AI governance. Identifying RAI weaknesses can also help secure funding for necessary initiatives.
Mitigating risks such as bias, discrimination, and privacy violations is paramount, and rigorous
RAI assessments enable proactive risk management, reducing potential harm to individuals and society.
The proposed RAI assessment supports regulatory compliance, risk mitigation, and continuous
improvement in AI governance and ethical responsibility.

×
  

AI practitioners often need to perform model comparison to select
the most suitable models for integration into larger pipelines, or to cross-check
fine-tuned models against their predecessors for desired improvements. Our goal
is to enhance the current model comparison pipeline in computer vision by eliminating
the need for exhaustive image annotation to collect validation sets, and by providing
more informative and actionable insights than conventional accuracy metrics. We are
developing a tool that leverages model disagreement signals as a search mechanism on
large, unlabeled data sources, thereby bypassing the need for human annotations. Our
tool identifies and produces "Error Pockets"—thematic groups of images where two
models disagree, which can be coherently described by humans. These error pockets
allow practitioners to examine the systematic strengths and weaknesses of any model
in the context of their specific application. This approach provides a deeper audit
beyond the simplistic analysis afforded by numerical metrics and offers prescriptive
data for model improvement with human-understandable explanations. By incorporating
sparse feedback from practitioners, our tool aligns error pocket searches with human
preferences. Our experiments demonstrate that training models on their identified
error pockets significantly improves performance on those specific error modes. We
are currently validating the effectiveness, understandability, and actionability of
error pockets through user studies with real AI practitioners who have various model comparison needs.

×
  

Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating
agents that control autonomous systems. However, the black-box nature of DRL agents limits
their deployment in real-world safety-critical applications. A promising approach for providing
strong guarantees on an agent's behavior is to use Neural Lyapunov Barrier (NLB) certificates,
which are learned functions over the system whose properties indirectly imply that an agent
behaves as desired. In this project, we explore the joint training of neural controllers and
NLB certificates. We use a formal neural network verification tool (Marabou) to check the certificate
and provide counterexample-guided feedback to the training. We also explore techniques for improving scalability
and compositionality and showcase our techniques on autonomous vehicle case studies.

×
  

Recently, 3D neural maps learned from sensor data have shown astounding results
in visual realism and detail, as well as more compactness and efficiency compared to traditional
map representations. However, despite their strengths over traditional map representations,
neural maps are still limited by incompletely observable and highly dynamic environments,
rendering their use in safety-critical systems challenging. We aim to leverage advancements in
neural scene reconstruction to develop high-quality neural mapping for environments ranging from
dense cities to extraterrestrial terrain. We are working to develop safe methods for downstream
navigation applications (localization, error forecasting, and path-planning) that are robust to
diverse sources of map errors. Additionally, we are working on methods to quantify and mitigate
uncertainty in map regions, provide bounds on localization error, and ensure safety guarantees for trajectory planning.

OUR FOCUS

### FORMAL METHODS

  

Using precise mathematical modeling to ensure the safety, security, and robustness of
conventional software and hardware systems.

### LEARNING AND CONTROL

  

Designing systems that intelligently balance learning under uncertainty and acting safety.

### TRANSPARENCY

  

Understanding safety in the context of fairness, accountability, and explainability for
autonomous and intelligent systems.

EVENTS

### 2025 AI Safety Annual Meeting

Monday, 22 September 2025, Paul Brest Hall, Stanford University

[LEARN MORE](annual-meeting-2025.html)
[REGISTER NOW](https://www.eventbrite.com/e/2025-ai-safety-annual-meeting-tickets-1442562847309)

Highlights from the previous annual meeting

[### AI Safety Seminar Series

  

Spring 2025](https://cs521.stanford.edu/)
[### AI Safety Annual Meeting

Summer 2024](https://www.cs.stanford.edu/events/affiliates-events/stanford-center-ai-safety-annual-meeting-2024)
[### AI Safety Annual Meeting

  

Summer 2023](https://www.cs.stanford.edu/events/affiliates-events/stanford-center-ai-safety-annual-meeting)

[### Explainability Workshop

  

Summer 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL)
[### AI Safety Annual Meeting

  

Summer 2022](https://datascience.stanford.edu/events/affiliate-program-event/day-1-ai-safety-workshop)
[### AI Safety Seminar Series

  

Spring 2022](https://cs521.stanford.edu/speaker-schedule/)

COURSES

[### PSYC 248: AI's Psyche and Psych: Mental Health in the Age of AI

  

Nina Vasan   
 Winter 2026](https://www.stanfordbrainstorm.com/courses)[### AA228V/CS238V - Validation of Safety Critical Systems

  

Sydney Katz   
 Winter 2025](https://aa228v.stanford.edu/)[### CS34 - Introduction to AI Governance

  

Max Lamparth   
 WInter 2025](https://web.stanford.edu/class/sts14/index.html)

[### AA275 - Navigation for Autonomous Systems

  

Grace Gao   
 Winter 2025](https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&q=AA%20275:%20Navigation%20for%20Autonomous%20Systems&academicYear=20202021)[### AA275 - Navigation for Autonomous Systems

  

Grace Gao   
 Winter 2025](https://web.stanford.edu/class/sts14/index.html)[### CS120 - Introduction to AI Safety

  

Max Lamparth   
 Fall 2024](https://web.stanford.edu/class/cs120/)

[### CS281 - Ethics of Artificial Intelligence

  

Carlos Guestrin   
 Spring 2024](https://stanfordaiethics.github.io/)[### CS221 - Artificial Intelligence

  

Sanmi Koyejo   
 Spring 2024](https://stanford-cs221.github.io/spring2024/)[### AA 273 - State Estimation and Filtering for Robotic Perception

  

Mac Schwager   
 Srping 2024](https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&q=AA%20273:%20State%20Estimation%20and%20Filtering%20for%20Aerospace%20Systems&academicYear=20182019)

[### AA 212 - Advanced Feedback Control Design

  

Mac Schwager   
 Winter 2023](https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=AA+212%3A+Advanced+Feedback+Control+Design&collapse=)[### AA 274B/CS 237B/EE 260B - Principles of Robot Autonomy II

  

Dorsa Sadigh & Marco Pavone   
 Winter 2023](http://web.stanford.edu/class/cs237b/)[### AA228/CS238 - Decision Making under Uncertainty

  

Mykel J. Kochenderfer   
 Fall 2023](https://aa228.stanford.edu/)

[### AA 274A/CS 237A/EE 260A - Principles of Robot Autonomy I

  

Mac Schwager   
 Fall 2023](https://stanfordasl.github.io//PoRA-I/aa274a_aut2324/)[### CS 333 - Safe and Interactive Robotics

  

Dorsa Sadigh   
 Winter 2022](https://dorsa.fyi/cs333/)[### CS 521 - Seminar On AI Safety

  

Anthony Corso   
 Spring 2022](https://cs521.stanford.edu/)

FEATURED PUBLICATIONS

  

[Show all publications](all-publications.html)

PEOPLE

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2d4d2f0963af161268cfaf9b554f407a.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/1fe3783a814613f4ad597ef14d0d3115.jpg)
  
  

Somil Bansal

Assistant Professor of Aeronautics and Astronautics

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c0a08ded65c70edadf7078f69b40ff26.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/ed24e7832ee935d311ac71edc8668a09.jpg)
  
  

Clark Barrett

Professor (Research) of Computer Science,   Co-Director

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/79bfa23cdc5a24cc3de85de88b1fd483.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c85742432a6cbc335f49fea8e8f084cc.jpg)
  
  

Jose Blanchet

Professor of Management Science and Engineering

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/a1412144ebfa07bec90e03938045596a.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/a311e3235827af31da58706d51619ebf.jpg)
  
  

Emma Brunskill

Associate Professor of Computer Science

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/73953e2161f246c67e2e72da1adc94ba.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/5f85d1f27094d475328155c5ff2f04ee.jpg)
  
  

David Dill

Donald E. Knuth Professor in the School of Engineering, Emeritus

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/3386b810e74616514fc7d8f5cc408a8c.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/d40288eb99cdf3f368b20917cb51ef15.jpg)
  
  

Charles Eesley

Associate Professor and W.M. Keck Foundation Faculty Scholar in the Department of Management Science and Engineering

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4339de052cecc41dd28758c7ff523391.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/6cfbeea5d75bb385ed274b3e00538979.jpg)
  
  

Paul Edwards

Director, Program on Science, Technology & Society, Co-Director,
Stanford Existential Risks Initiative, Professor of Information and History (Emeritus),
University of Michigan

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/39469282ac55afd3560db9310f5befad.png)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/567fc7ad49c001031b46c29f45d5bc2e.jpg)
  
  

Grace X. Gao

Associate Professor of Aeronautics and Astronautics, and by
courtesy, Electrical Engineering, Co-Director

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/e9c1be8bd3870cff11417b271c3317e6.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/63d375f46e1f0cfc6b3aed34d1407a22.jpg)
  
  

Carlos Guestrin

Associate Professor of Computer Science

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2fcb461c92776e123f7ab6d681af8527.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2466576effa594b488b0b07fbdedd7a7.jpg)
  
  

Mykel Kochenderfer

Associate Professor of Aeronautics and Astronautics and, by
courtesy, Computer Science, Co-Director

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/1246eea3a8c42013b50052df6cc932be.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/52c552d2334b22112cbe10630c6e3d07.jpg)
  
  

Sanmi Koyejo

Assistant Professor of Computer Science

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/77d50207ade46658076df28d26263b29.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/47464e5150c8a2433f8d5e3bb9e23f58.jpg)
  
  

Steve Luby

Professor of Medicine (Infectious Diseases & Geographic Medicine),
Co-Director, Stanford Existential Risks Initiative

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/92905bd2dfb9947948ce86b40af9c495.jpeg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/67acb0e94fa2987d222e78681ef63367.jpg)
  
  

Azalia Mirhoseini

Assistant Professor of Computer Science

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f0a4d932707652bf5a222bc473e9774c.jpg)](h://web.stanford.edu/~pavone/)
  
  

Marco Pavone

Associate Professor of Aeronautics and Astronautics and, by
courtesy, of Electrical Engineering, of Computational and Mathematical Engineering, and of
Information Systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/836e54e995765b07e33a9da014ab74f5.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/e7e79def3549573953dd49f27eaf3fda.jpg)
  
  

Dorsa Sadigh

Assistant Professor of Computer Science and of Electrical
Engineering

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4f704bac94b53355eedbf4134ca1a4f4.png)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c2783a6d3c4863db7de7826ec16444e0.jpg)
  
  

Mac Schwager

Assistant Professor of Aeronautics and Astronautics, Director of the
Multi-robot Systems Lab

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2abb2b172ee75b0d78411765dbf8c7eb.png)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/fa7006b269980157277ddfe154b45062.jpg)
  
  

Nina Vasan

Clinical Assistant Professor of Psychiatry

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/31d52364b37dcf98fbaa9a6931b1a4e9.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f257044c7d1602395c003ad20b46253d.jpg)
  
  

Madeleine Udell

Assistant Professor, Management Science and Engineering and, by courtesy, Electrical Engineering

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f7911ebef21520954207a95c1e458da0.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/fbafd40f6f6017f2d6189e0b2156a16b.html)
  
  

Diyi Yang

Assistant Professor of Computer Science

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2ba7b30fb01f3693de7f2c02542889fe.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/ceeeff725cc683634929522c6d496ac8.jpg)
  
  

James Zou

Associate Professor of Biomedical Data Science and, by courtesy, of
Computer Science and of Electrical Engineering

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/80afacc892fa29e51f4457829ee5a7b4.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/d7fdc241d4f877e04c422a3bc84bac2f.jpg)
  
  

Mansur Arief

Executive Director

AI for safety and sustainability

  

POSTDOCTORAL SCHOLARS AND RESEARCH FELLOWS



[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/aa2a0c49b5df9b9d008d77e7d9345ff6.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/5fab918c1bd5db6c021770bf906a00d3.jpg)
  
  

Duncan Eddy

Postdoctoral Scholar

Learning-based failure testing of cyberphysical systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/7d20035182d96fcb3a6f9ab8c13614cd.jpeg)](#)
  
  

Pei Huang

Postdoctoral Scholar

Automated reasoning and trustworthy AI

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/74127d8550105ac0c235ee9786aa1604.jpg)](#)
  
  

Kiana Jafari

Postdoctoral Scholar

Human-centric artificial intelligence

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4a78069e72aca25ecd27fd08dd25e4f1.jpg)](#)
  
  

Sydney Katz

Postdoctoral Scholar

Verification of autonomous systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/e8f59ac0d1019ca415b34176a8bbd446.jpg)](#)
  
  

Max Lamparth

Postdoctoral Scholar

Interpretability, robustness and safety of language models

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/fab7a4092d23d96b67f45066bd37f4dd.jpg)](#)
  
  

Robert J. Moss

Postdoctoral Scholar

Risk assessment of autonomous systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/d5211e1151f609326bd8ac8a0bddea47.png)](#)
  
  

Kemal Nazim Ure

Visiting Scholar

Robust planning for autonomous systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/8fc87fabc20076fca0d413daf854afe1.jpg)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/807a3514b669e830fa84e6a7ecf286e0.jpg)
  
  

Min Wu

Postdoctoral Scholar

AI safety: robustness, explainability, and fairness

  

GRADUATE STUDENTS

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4982eb7e46dda279b33c438271bbecbe.jpg)](#)
  
  

Chris Agia

Ph.D. Candidate

Robot learning, integrated task & motion planning

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/33b271bcccf3d764dcd39814298cc57a.jpg)](#)
  
  

Wajeeha Ahmad

Ph.D. Candidate

Impact of AI on misinformation and web content quality

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/641baece5c78ee8a577a380fe175845e.jpg)](#)
  
  

Samuel Akinwande

Ph.D. Candidate

Formal verification of nonlinear systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/637209b2c2ec345d55ec07abe0520395.png)](#)
  
  

Suneel Belkhale

Ph.D. Candidate

Robot perception and decision making

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c40da2bfd72c2c45ef3aaca45e7ff243.jpeg)](#)
  
  

Polo Contreras

Ph.D. Candidate

Multi-robot systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/20563943fc6129b4de39881863b28df7.jpeg)](#)
  
  

Tim Chen

Ph.D. Candidate

Safety in Learned Environments

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/0cf8beecddc5af0d66593e269eeeadf5.jpg)](#)
  
  

Kaila Coimbra

Ph.D. Candidate

Lunar surface rover positioning

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f690480628c9721e34541f0fdd8ec429.jpg)](#)
  
  

Marta Cortinovis

Ph.D. Candidate

Lunar navigation ephemeris modeling

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/3916299c2853ef5dc9e6331b3d475bab.jpg)](#)
  
  

Adam Dai

Ph.D. Candidate

Provably safe trajectory planning under uncertainty

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c6d40ec0fbae63a75b49de2c24a4f971.jpg)](#)
  
  

Amine Elhafsi

Ph.D. Candidate

Planning and control for safe robotic navigation

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/de349e2534a51f6f608bb73db5e436c3.jpg)](#)
  
  

Aaron Feldman

Ph.D. Candidate

Statistical performance guarantees

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/932ba1274a68f07febfbfc07dc01a036.jpg)](#)
  
  

Zeyuan Feng

Ph.D. Candidate

Formal safety for machine learning in robotics

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4f1e4e24cc61af28dca33e6e6d086597.jpg)](#)
  
  

Matt Foutter

Ph.D. Candidate

Safe and reliable autonomy

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/41e4d9a6313e23c95e5431a6d2a7ac42.jpg)](#)
  
  

Aryaman Gupta

Ph.D. Candidate

Safe and robust learning-based control

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/0e26dfe8ae6b418bbe1e0fd731aceec9.jpg)](#)
  
  

Amelia Hardy

M.S. Candidate

Constrained decision problems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f374506a473c5b45a41786c762df1771.jpg)](#)
  
  

Keidai Iiyama

Ph.D. Candidate

Navigation for the Moon

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/6901bc9a4122c7c9e147dce31378f402.png)](#)
  
  

Albert Lin

Ph.D. Candidate

Machine learning for robot control and safety

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c3978ce36965c913133b01ea23165a6b.jpg)](#)
  
  

Udayan Mandal

M.S. Candidate

Formal verification

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4bb36ae21c44b96bec4f069104dea46e.jpeg)](#)
  
  

Daniel Neamati

Ph.D. Candidate

Urban GNSS-based navigation using environmental features

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/3ba3b5c8d01e0be7578ab1d322c3a96d.jpg)](#)
  
  

Shreya Parjan

M.S. Candidate

Safety validation for cyber-physical systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/17a784c7dec4e1534921b90944deb979.jpeg)](#)
  
  

Mira Partha

Ph.D. Candidate

Neural Radiance Field (NeRF) maps of urban Environments

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/8bed99617016670f4d62990e18b9edd5.jpg)](#)
  
  

Ann-Katrin Reuel

Ph.D. Candidate

Ethics of automated driving

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/180e3404a2d55de2f10933a6f9e614fe.jpg)](#)
  
  

Marc Schlichting

Ph.D. Candidate

Machine learning for safety validation

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2efdbf99da2f2d65ff4c82c24d535193.png)](#)
  
  

Rohan Sinha

Ph.D. Candidate

Trustworthy autonomy and out-of-distribution generalization

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/8fbb9c5b264d9721eafdf212dffcb1af.png)](#)
  
  

Megha Srivastava

Ph.D. Candidate

Reliability, security, and robustness through the lens of human-AI
interaction and collaboration

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/625aa1e4edd4f51b6d60b3a90af03105.png)](#)
  
  

Jiankai Sun

Ph.D. Candidate

Reliable and efficient AI systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/50fac43c911150ff6e83f0a9a431d2b1.jpg)](#)
  
  

Romeo Valentin

Ph.D. Candidate

Uncertainty calibration of neural networks

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/fe329c70a3758356b0da06f7841ccd2e.jpg)](#)
  
  

Guillem Casadesus Vila

Ph.D. Candidate

Lunar positioning, navigation, and timing

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/2099833c9e20554a6dc9e4058c91e87d.jpg)](#)
  
  

Scott Viteri

Ph.D. Candidate

AI alignment via ontology maps

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/408190eb04acb5af52f8902f7d02e1ab.jpg)](#)
  
  

Hao Wang

Ph.D. Candidate

Safety of foundation model-enabled autonomous systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f3c4ee59a42614a041606fcabe9f1467.jpg)](#)
  
  

Jun Wang

M.S. Candidate

Autonomous agent falsification

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/577eeac2a6a739fccbbb2f4305265831.jpg)](#)
  
  

Hailey Warner

Ph.D. Candidate

Safety validation for aerospace systems

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/b0d1ddb7a50291abea99e78a4df63bd6.jpg)](#)
  
  

Isaac Ward

Ph.D. Candidate

Model predictive control for safety critical systems

[![person](images/asta-wu.webp)](#)
  
  

Asta Wu

Ph.D. Candidate

GNSS-based relative localization for cars

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/e487870792424794329141aeae77f92f.jpg)](#)
  
  

Haoze Wu

Ph.D. Candidate

Formal verification

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/f12615d624bdaadcaaceec05e934a268.jpeg)](#)
  
  

Alan Yang

Ph.D. Candidate

GNSS code design using AI and machine learning

[![person](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/20c0e3306460a3b7d3ac7198fe6dec10.png)](#)
  
  

Yibo Zhang

Ph.D. Candidate

AI safety from a fundamental perspective

MEMBERSHIP

Our corporate members are a vital and integral part of the Center for AI Safety. They provide
insight on real-world use cases, valuable financial support for research, and a path to
large-scale impact.

  
  

Benefits | Core | Associate || Sponsorship level per year | 300k | 100k |
| Commitment | yearly\* | yearly |
| Sponsorship of a single research project collaboratively-defined with a Center researchers | Yes | Yes |
| Access to student recruiting opportunities | Yes | Yes |
| Annual research symposium invitations | Yes | Yes |
| Research seminar participation | Yes | Yes |
| Discounted employee enrollments in select [professional and technical courses](https://online.stanford.edu/professional-education) from the Stanford Engineering Center for Global and Online Education | Yes | Yes |
| Visiting scholar positions\*\* | Yes | Additional fee |
| Opportunity to contribute to the definition of a  flagship research project involving multiple faculty and students | Yes | No |
| Faculty visits\*\*\* | Yes | No |

  
  

The Stanford Center for AI Safety researchers will use and develop open-source software, and it is the intention of all Center for AI Safety
researchers that any software released will be released under an open source model.

Affiliate Program members may provide additional funding. All research results arising from the use of the
additional funding will be shared with all program members and the general public. Affiliate Program
members may request the additional funding be used to support a particular area of program research
identified on the program’s website, or the program research of a named faculty member, as long as the
faculty is identified on the program website as participating in the Affiliate Program. In either instance, the
director of the Affiliate Program will determine how the additional funding will be used in the program’s
research.

\* The desired commitment is for 3 years to ensure continuity of funding and successful completion of research. Membership is renewed annually.

\*\* For additional information please see the [Visiting Scholar Policy](https://doresearch.stanford.edu/policies/research-policy-handbook/non-faculty-research-appointments/visiting-scholars)

\*\*\* The site presentations and all information, data and results arising from such visitation interactions will be shared with all members and the public.

  

For more information, please view our [Corporate Membership
Document](membershipdoc2024.pdf)   
  
To view Stanford policies, please see [Stanford
University Policies for Industrial Affiliate Programs](https://doresearch.stanford.edu/policies/research-policy-handbook/definitions-and-types-agreements/establishment-industrial-affiliates-and-related-membership-supported-programs)

  
![](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/19fa6c76eaeb503c09579c5d2b54de1f.png)

CONTACT

Mansur Arief

Executive Director - Center for AI Safety

MAILING LIST

## Subscribe to Stanford Center for AI Safety Newsletter and Announcements

\* indicates required

Email Address \*

First Name \*

Last Name \*

Company \*

If no company association put N/A

[View previous newsletters](https://us21.campaign-archive.com/home/?u=2ae6092e57ebc2c42434250e8&id=95d39a64be)

We are grateful to our affiliates program members for their generous support:  
  
**Core Members:**  
  
![group](images/torc_robotics.webp)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/39c86baa7e10a5189ddefc888367a093.png)
  
  




**Associate Members:**  
  
![group](images/nri_brand_mark.svg)
  
  
**Federal Sponsors:**  
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/132c7b5f57aede1d25fa82b37d3c51fa.png)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/0cdbca638f8adc07be239aae2f1c43d7.png)
  
  
**Additional Sponsors:**  
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/44be6ead383004a65157749a61d2f051.jpg)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/b9f08568022c727e9159ca771e7ed4a2.png)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/c4e138533220d8df5104c50569bb1f00.png)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/50c2907f540e66f91d208cb28b8a95ea.png)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4cf42d5b55c4f05711f4d83cab4f6592.png)
![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/996ee14ca2ccb7dee02383a05127870d.png)
  
  
**Affiliated Organizations:**  
  
  

[![group](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/4954ed4eadb38c1c07637c3e3206f44b.png)](ai-leaders-insights/安全治理与未来/AI安全/安全研究/images/99ffd2a3f54a6d3908d9b7fa84450eb3.jpg)

[Stanford
Existential  
 Risks Initiative (SERI)](https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative)

---

*爬取时间: 2025-11-28 21:57:18*
