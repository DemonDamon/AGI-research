<!doctype html>
<html xmlns:og="http://opengraphprotocol.org/schema/" xmlns:fb="http://www.facebook.com/2008/fbml" lang="en-GB" >
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <!-- This is Squarespace. --><!-- tobyord -->
<base href="">
<meta charset="utf-8" />
<title>Inference Scaling Reshapes AI Governance &mdash; Toby Ord</title>
<meta http-equiv="Accept-CH" content="Sec-CH-UA-Platform-Version, Sec-CH-UA-Model" /><link rel="icon" type="image/x-icon" href="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/1582714470910-7TPWUYOGDEREDJ772NHV/favicon.ico"/>
<link rel="canonical" href="https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance"/>
<meta property="og:site_name" content="Toby Ord"/>
<meta property="og:title" content="Inference Scaling Reshapes AI Governance &mdash; Toby Ord"/>
<meta property="og:latitude" content="40.7207559"/>
<meta property="og:longitude" content="-74.0007613"/>
<meta property="og:locality" content=""/>
<meta property="og:url" content="https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="The shift from scaling up the pre-training compute of AI systems to scaling up their inference compute may have profound effects on AI governance. The nature of these effects depends crucially on whether this new inference compute will primarily be used during external deployment or as part of a mor"/>
<meta property="og:image" content="http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w"/>
<meta property="og:image:width" content="1500"/>
<meta property="og:image:height" content="760"/>
<meta itemprop="name" content="Inference Scaling Reshapes AI Governance — Toby Ord"/>
<meta itemprop="url" content="https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance"/>
<meta itemprop="description" content="The shift from scaling up the pre-training compute of AI systems to scaling up their inference compute may have profound effects on AI governance. The nature of these effects depends crucially on whether this new inference compute will primarily be used during external deployment or as part of a mor"/>
<meta itemprop="thumbnailUrl" content="http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w"/>
<link rel="image_src" href="http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w" />
<meta itemprop="image" content="http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w"/>
<meta itemprop="author" content="Toby Ord"/>
<meta itemprop="datePublished" content="2025-02-12T12:31:07+0000"/>
<meta itemprop="dateModified" content="2025-06-23T11:52:02+0100"/>
<meta itemprop="headline" content="Inference Scaling Reshapes AI Governance"/>
<meta itemprop="publisher" content="Toby Ord"/>
<meta name="twitter:title" content="Inference Scaling Reshapes AI Governance — Toby Ord"/>
<meta name="twitter:image" content="http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w"/>
<meta name="twitter:url" content="https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:description" content="The shift from scaling up the pre-training compute of AI systems to scaling up their inference compute may have profound effects on AI governance. The nature of these effects depends crucially on whether this new inference compute will primarily be used during external deployment or as part of a mor"/>
<meta name="description" content="" />
<link rel="preconnect" href="https://images.squarespace-cdn.com">
<link rel="preconnect" href="https://use.typekit.net" crossorigin>
<link rel="preconnect" href="https://p.typekit.net" crossorigin>
<script type="text/javascript" src="//use.typekit.net/ik/fBxt2KNC8FHb6oiw3EoNU0pyYFeYrrdGZqubwsNf233feltIfFHN4UJLFRbh52jhWD9hwRjuwcsKZQsKw2mKjcZqjD9oFcZqFUnTMKG0jAFu-WsoShFGZAsude80ZkoRdhXCHKoyjamTiY8Djhy8ZYmC-Ao1Oco8if37OcBDOcu8OfG0SaBujW48SagyjhmDjhy8ZYmC-Ao1OcFzdP37O1szj18zScb0SaBujW48Sagyjh90jhNlOeBRiA8XpWFR-emqiAUTdcS0jhNlOeBRiA8XpWFR-emqiAUTdcS0dcmXOeBDOcu8OeUzjhBC-eNDifUDSWmyScmDSeBRZWFR-emqiAUTdcS0jhNlOYszj18zScb0jhNlOYszj18zScb0SaBujW48Sagyjh90jhNlOYiaikoyjamTiY8Djhy8ZYmC-Ao1OcFzdPUaiaS0jAFu-WsoShFGZAsude80Zko0ZWbCiaiaOcBDOcu8OYiaikoDSWmyScmDSeBRZWFR-emqiAUTdcS0jhNlOYiaikoXdh4oda4qOcFzdPUaiaS0ieNKpANkZfoDSWmyScmDSeBRZPoRdhXK2YgkdayTdAIldcNhjPJPjAszjc9lZhBkjAuzdcblSY4zJ6Z8iW4zSeI7fbKnMsMMeMI6MKG4f5J7IMMjMkMfH6qJnbIbMg6eJMJ7fbKOMsMMeMS6MKG4f5w7IMMj2PMfH6qJRMIbMg6sJMJ7fbRDFgMgeMb6MKG4fVMXIMIjgkMfH6qJvRbbMs65JMJ7fbRUFgMgegI6MKG4fHToIMJjgfMfH6qJ7YqbMy6YJMJ7f6Rqy6IbMy65JMJ7f6R8y6IbMy6sJMHbMieb37Me.js" async fetchpriority="high" onload="try{Typekit.load();}catch(e){} document.documentElement.classList.remove('wf-loading');"></script>
<script>document.documentElement.classList.add('wf-loading')</script>
<style>@keyframes fonts-loading { 0%, 99% { color: transparent; } } html.wf-loading * { animation: fonts-loading 3s; }</style>
<script type="text/javascript" crossorigin="anonymous" nomodule="nomodule" src="//assets.squarespace.com/@sqs/polyfiller/1.6/legacy.js"></script>
<script type="text/javascript" crossorigin="anonymous" src="//assets.squarespace.com/@sqs/polyfiller/1.6/modern.js"></script>
<script type="text/javascript">SQUARESPACE_ROLLUPS = {};</script>
<script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/extract-css-runtime-9955ad5995a2e1ec-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-extract_css_runtime');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/extract-css-runtime-9955ad5995a2e1ec-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/extract-css-moment-js-vendor-6f2a1f6ec9a41489-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-extract_css_moment_js_vendor');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/extract-css-moment-js-vendor-6f2a1f6ec9a41489-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/cldr-resource-pack-22ed584d99d9b83d-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-cldr_resource_pack');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/cldr-resource-pack-22ed584d99d9b83d-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-vendors-stable-fbd854d40b0804b7-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common_vendors_stable');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-vendors-stable-fbd854d40b0804b7-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-vendors-521b3d1ea2c5615b-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common_vendors');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-vendors-521b3d1ea2c5615b-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-bff30ef0ecfbeb97-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-bff30ef0ecfbeb97-min.en-US.js" ></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/performance-cba60b5f3aa909d1-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-performance');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/performance-cba60b5f3aa909d1-min.en-US.js" defer ></script><script data-name="static-context">Static = window.Static || {}; Static.SQUARESPACE_CONTEXT = {"betaFeatureFlags":["campaigns_merch_state","i18n_beta_website_locales","marketing_landing_page","campaigns_discount_section_in_blasts","campaigns_new_image_layout_picker","order_status_page_checkout_landing_enabled","enable_form_submission_trigger","campaigns_import_discounts","bfcm_2025_enabled","use_react_flow_in_automations_flowchart","contacts_and_campaigns_redesign","marketing_automations","campaigns_thumbnail_layout","campaigns_discount_section_in_automations","use_chained_automations_data"],"facebookAppId":"314192535267336","facebookApiVersion":"v6.0","rollups":{"squarespace-announcement-bar":{"js":"//assets.squarespace.com/universal/scripts-compressed/announcement-bar-18f27e520beb2bd2-min.en-US.js"},"squarespace-audio-player":{"css":"//assets.squarespace.com/universal/styles-compressed/audio-player-b05f5197a871c566-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/audio-player-66bea742780d3b46-min.en-US.js"},"squarespace-blog-collection-list":{"css":"//assets.squarespace.com/universal/styles-compressed/blog-collection-list-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/blog-collection-list-f78db80fc1cd6fce-min.en-US.js"},"squarespace-calendar-block-renderer":{"css":"//assets.squarespace.com/universal/styles-compressed/calendar-block-renderer-b72d08ba4421f5a0-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/calendar-block-renderer-d588b06721fbdbb5-min.en-US.js"},"squarespace-chartjs-helpers":{"css":"//assets.squarespace.com/universal/styles-compressed/chartjs-helpers-96b256171ee039c1-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/chartjs-helpers-4fd57f343946d08e-min.en-US.js"},"squarespace-comments":{"css":"//assets.squarespace.com/universal/styles-compressed/comments-d4212e6832aeb2e3-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/comments-df03d6fde412fb5a-min.en-US.js"},"squarespace-custom-css-popup":{"css":"//assets.squarespace.com/universal/styles-compressed/custom-css-popup-72c4bbed7f21a6e6-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/custom-css-popup-892f952ae4f30456-min.en-US.js"},"squarespace-dialog":{"css":"//assets.squarespace.com/universal/styles-compressed/dialog-f9093f2d526b94df-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/dialog-60cbb5513421d519-min.en-US.js"},"squarespace-events-collection":{"css":"//assets.squarespace.com/universal/styles-compressed/events-collection-b72d08ba4421f5a0-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/events-collection-a3ad933f9c4ad26c-min.en-US.js"},"squarespace-form-rendering-utils":{"js":"//assets.squarespace.com/universal/scripts-compressed/form-rendering-utils-3b8b35485f6ec7cd-min.en-US.js"},"squarespace-forms":{"css":"//assets.squarespace.com/universal/styles-compressed/forms-0afd3c6ac30bbab1-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/forms-1f40e50b4e4b4da6-min.en-US.js"},"squarespace-gallery-collection-list":{"css":"//assets.squarespace.com/universal/styles-compressed/gallery-collection-list-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/gallery-collection-list-07747667a3187b76-min.en-US.js"},"squarespace-image-zoom":{"css":"//assets.squarespace.com/universal/styles-compressed/image-zoom-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/image-zoom-60c18dc5f8f599ea-min.en-US.js"},"squarespace-pinterest":{"css":"//assets.squarespace.com/universal/styles-compressed/pinterest-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/pinterest-c4e3cee5f0f05a76-min.en-US.js"},"squarespace-popup-overlay":{"css":"//assets.squarespace.com/universal/styles-compressed/popup-overlay-b742b752f5880972-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/popup-overlay-8e28db39a5796b9b-min.en-US.js"},"squarespace-product-quick-view":{"css":"//assets.squarespace.com/universal/styles-compressed/product-quick-view-9052fb446c5800ee-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/product-quick-view-12cdb153a5692867-min.en-US.js"},"squarespace-products-collection-item-v2":{"css":"//assets.squarespace.com/universal/styles-compressed/products-collection-item-v2-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/products-collection-item-v2-e3a3f101748fca6e-min.en-US.js"},"squarespace-products-collection-list-v2":{"css":"//assets.squarespace.com/universal/styles-compressed/products-collection-list-v2-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/products-collection-list-v2-eedc544f4cc56af4-min.en-US.js"},"squarespace-search-page":{"css":"//assets.squarespace.com/universal/styles-compressed/search-page-90a67fc09b9b32c6-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/search-page-e70c98bf6dd7112c-min.en-US.js"},"squarespace-search-preview":{"js":"//assets.squarespace.com/universal/scripts-compressed/search-preview-e4bc2f62b00ef9cf-min.en-US.js"},"squarespace-simple-liking":{"css":"//assets.squarespace.com/universal/styles-compressed/simple-liking-701bf8bbc05ec6aa-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/simple-liking-e6bc64da9e3b1cf4-min.en-US.js"},"squarespace-social-buttons":{"css":"//assets.squarespace.com/universal/styles-compressed/social-buttons-95032e5fa98e47a5-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/social-buttons-4d0f66a6979eafb0-min.en-US.js"},"squarespace-tourdates":{"css":"//assets.squarespace.com/universal/styles-compressed/tourdates-b4046463b72f34e2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/tourdates-121ba0b703ba2a1f-min.en-US.js"},"squarespace-website-overlays-manager":{"css":"//assets.squarespace.com/universal/styles-compressed/website-overlays-manager-07ea5a4e004e6710-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/website-overlays-manager-3506d37a1fad4643-min.en-US.js"}},"pageType":50,"website":{"id":"562652dbe4b05bbfdc596fd7","identifier":"tobyord","websiteType":1,"contentModifiedOn":1764332621727,"cloneable":false,"hasBeenCloneable":false,"developerMode":false,"siteStatus":{},"language":"en-GB","translationLocale":"en-US","formattingLocale":"en-US","timeZone":"Europe/London","machineTimeZoneOffset":0,"timeZoneOffset":0,"timeZoneAbbr":"GMT","siteTitle":"Toby Ord","fullSiteTitle":"Inference Scaling Reshapes AI Governance \u2014 Toby Ord","siteDescription":"<p>An Oxford philosopher working on big picture questions facing humanity this century.</p>","shareButtonOptions":{"3":true,"1":true,"8":true,"4":true,"2":true,"7":true,"6":true},"authenticUrl":"https://www.tobyord.com","internalUrl":"https://tobyord.squarespace.com","baseUrl":"https://www.tobyord.com","primaryDomain":"www.tobyord.com","sslSetting":3,"isHstsEnabled":false,"socialAccounts":[{"serviceId":20,"userId":"toby.ord@philosophy.ox.ac.uk","screenname":"toby.ord@philosophy.ox.ac.uk","addedOn":1445444897615,"profileUrl":"mailto:toby.ord@philosophy.ox.ac.uk","iconEnabled":true,"serviceName":"email"}],"typekitId":"","statsMigrated":false,"imageMetadataProcessingEnabled":false,"screenshotId":"c0b1847868b995cf10dcc059325d0d61f438c82575aaea3520d4d24021d1756d","captchaSettings":{"enabledForDonations":false},"showOwnerLogin":false},"websiteSettings":{"id":"562652dbe4b05bbfdc596fda","websiteId":"562652dbe4b05bbfdc596fd7","type":"Personal","subjects":[],"country":"GB","state":"ENG","simpleLikingEnabled":true,"mobileInfoBarSettings":{"style":1,"isContactEmailEnabled":false,"isContactPhoneNumberEnabled":false,"isLocationEnabled":false,"isBusinessHoursEnabled":false},"announcementBarSettings":{"text":""},"popupOverlaySettings":{"style":1,"enabledPages":[]},"commentLikesAllowed":true,"commentAnonAllowed":true,"commentThreaded":true,"commentApprovalRequired":false,"commentAvatarsOn":true,"commentSortType":2,"commentFlagThreshold":0,"commentFlagsAllowed":true,"commentEnableByDefault":true,"commentDisableAfterDaysDefault":0,"disqusShortname":"","commentsEnabled":false,"contactPhoneNumber":"","storeSettings":{"returnPolicy":null,"termsOfService":null,"privacyPolicy":null,"expressCheckout":false,"continueShoppingLinkUrl":"/","useLightCart":false,"showNoteField":false,"shippingCountryDefaultValue":"US","billToShippingDefaultValue":false,"showShippingPhoneNumber":true,"isShippingPhoneRequired":false,"showBillingPhoneNumber":true,"isBillingPhoneRequired":false,"currenciesSupported":["USD","CAD","GBP","AUD","EUR","CHF"],"defaultCurrency":"USD","selectedCurrency":"USD","measurementStandard":1,"showCustomCheckoutForm":false,"checkoutPageMarketingOptInEnabled":false,"enableMailingListOptInByDefault":true,"sameAsRetailLocation":false,"merchandisingSettings":{"scarcityEnabledOnProductItems":false,"scarcityEnabledOnProductBlocks":false,"scarcityMessageType":"DEFAULT_SCARCITY_MESSAGE","scarcityThreshold":10,"multipleQuantityAllowedForServices":true,"restockNotificationsEnabled":false,"restockNotificationsSuccessText":"","restockNotificationsMailingListSignUpEnabled":false,"relatedProductsEnabled":false,"relatedProductsOrdering":"random","soldOutVariantsDropdownDisabled":false,"productComposerOptedIn":false,"productComposerABTestOptedOut":false,"productReviewsEnabled":false,"displayImportedProductReviewsEnabled":false,"hasOptedToCollectNativeReviews":false},"minimumOrderSubtotalEnabled":false,"isLive":false,"multipleQuantityAllowedForServices":true},"useEscapeKeyToLogin":true,"ssBadgeType":1,"ssBadgePosition":4,"ssBadgeVisibility":1,"ssBadgeDevices":1,"pinterestOverlayOptions":{"mode":"disabled"},"userAccountsSettings":{"loginAllowed":false,"signupAllowed":false}},"cookieSettings":{"isCookieBannerEnabled":false,"isRestrictiveCookiePolicyEnabled":false,"cookieBannerText":"","cookieBannerTheme":"","cookieBannerVariant":"","cookieBannerPosition":"","cookieBannerCtaVariant":"","cookieBannerCtaText":"","cookieBannerAcceptType":"OPT_IN","cookieBannerOptOutCtaText":"","cookieBannerHasOptOut":false,"cookieBannerHasManageCookies":true,"cookieBannerManageCookiesLabel":"","cookieBannerSavedPreferencesText":"","cookieBannerSavedPreferencesLayout":"PILL"},"websiteCloneable":false,"collection":{"title":"Writing","id":"605a132f7a36283cfa0c4af9","fullUrl":"/writing","type":1,"permissionType":1},"item":{"title":"Inference Scaling Reshapes AI Governance","id":"67ac8d877fb3420883967ac0","fullUrl":"/writing/inference-scaling-reshapes-ai-governance","publicCommentCount":0,"commentState":2,"recordType":1},"subscribed":false,"appDomain":"squarespace.com","templateTweakable":true,"tweakJSON":{"aspect-ratio":"Auto","bannerImageHeight":"540px","gallery-arrow-style":"No Background","gallery-aspect-ratio":"3:2 Standard","gallery-auto-crop":"true","gallery-autoplay":"true","gallery-design":"Slideshow","gallery-info-overlay":"Show on Hover","gallery-loop":"true","gallery-navigation":"Circles","gallery-show-arrows":"false","gallery-transitions":"Scroll","galleryArrowBackground":"rgba(34,34,34,1)","galleryArrowColor":"rgba(255,255,255,1)","galleryAutoplaySpeed":"3","galleryCircleColor":"rgba(232,232,232,1)","galleryInfoBackground":"rgba(0, 0, 0, .7)","galleryThumbnailSize":"100px","gridSize":"350px","gridSpacing":"10px","product-gallery-auto-crop":"true","product-image-auto-crop":"true","tweak-v1-related-products-title-spacing":"50px"},"templateId":"5093f261e4b0979eac7cb299","templateVersion":"7","pageFeatures":[1,2,4],"gmRenderKey":"QUl6YVN5Q0JUUk9xNkx1dkZfSUUxcjQ2LVQ0QWVUU1YtMGQ3bXk4","templateScriptsRootUrl":"https://static1.squarespace.com/static/ta/5093f258e4b0979eac7cb197/3323/scripts/","impersonatedSession":false,"tzData":{"zones":[[0,"EU","GMT/BST",null]],"rules":{"EU":[[1981,"max",null,"Mar","lastSun","1:00u","1:00","S"],[1996,"max",null,"Oct","lastSun","1:00u","0",null]]}},"showAnnouncementBar":false,"recaptchaEnterpriseContext":{"recaptchaEnterpriseSiteKey":"6LdDFQwjAAAAAPigEvvPgEVbb7QBm-TkVJdDTlAv"},"i18nContext":{"timeZoneData":{"id":"Europe/London","name":"Greenwich Mean Time"}},"env":"PRODUCTION","visitorFormContext":{"formFieldFormats":{"initialNameOrder":"GIVEN_FIRST","initialAddressFormat":{"id":0,"type":"ADDRESS","country":"GB","labelLocale":"en","fields":[{"type":"FIELD","label":"Address Line 1","identifier":"Line1","length":0,"required":true,"metadata":{"autocomplete":"address-line1"}},{"type":"SEPARATOR","label":"\n","identifier":"Newline","length":0,"required":false,"metadata":{}},{"type":"FIELD","label":"Address Line 2","identifier":"Line2","length":0,"required":false,"metadata":{"autocomplete":"address-line2"}},{"type":"SEPARATOR","label":"\n","identifier":"Newline","length":0,"required":false,"metadata":{}},{"type":"FIELD","label":"City / Town","identifier":"City","length":0,"required":true,"metadata":{"autocomplete":"address-level1"}},{"type":"SEPARATOR","label":"\n","identifier":"Newline","length":0,"required":false,"metadata":{}},{"type":"FIELD","label":"Postcode","identifier":"Zip","length":0,"required":true,"metadata":{"autocomplete":"postal-code"}}]},"countries":[{"name":"Afghanistan","code":"AF","phoneCode":"+93"},{"name":"\u00C5land Islands","code":"AX","phoneCode":"+358"},{"name":"Albania","code":"AL","phoneCode":"+355"},{"name":"Algeria","code":"DZ","phoneCode":"+213"},{"name":"American Samoa","code":"AS","phoneCode":"+1"},{"name":"Andorra","code":"AD","phoneCode":"+376"},{"name":"Angola","code":"AO","phoneCode":"+244"},{"name":"Anguilla","code":"AI","phoneCode":"+1"},{"name":"Antigua & Barbuda","code":"AG","phoneCode":"+1"},{"name":"Argentina","code":"AR","phoneCode":"+54"},{"name":"Armenia","code":"AM","phoneCode":"+374"},{"name":"Aruba","code":"AW","phoneCode":"+297"},{"name":"Ascension Island","code":"AC","phoneCode":"+247"},{"name":"Australia","code":"AU","phoneCode":"+61"},{"name":"Austria","code":"AT","phoneCode":"+43"},{"name":"Azerbaijan","code":"AZ","phoneCode":"+994"},{"name":"Bahamas","code":"BS","phoneCode":"+1"},{"name":"Bahrain","code":"BH","phoneCode":"+973"},{"name":"Bangladesh","code":"BD","phoneCode":"+880"},{"name":"Barbados","code":"BB","phoneCode":"+1"},{"name":"Belarus","code":"BY","phoneCode":"+375"},{"name":"Belgium","code":"BE","phoneCode":"+32"},{"name":"Belize","code":"BZ","phoneCode":"+501"},{"name":"Benin","code":"BJ","phoneCode":"+229"},{"name":"Bermuda","code":"BM","phoneCode":"+1"},{"name":"Bhutan","code":"BT","phoneCode":"+975"},{"name":"Bolivia","code":"BO","phoneCode":"+591"},{"name":"Bosnia & Herzegovina","code":"BA","phoneCode":"+387"},{"name":"Botswana","code":"BW","phoneCode":"+267"},{"name":"Brazil","code":"BR","phoneCode":"+55"},{"name":"British Indian Ocean Territory","code":"IO","phoneCode":"+246"},{"name":"British Virgin Islands","code":"VG","phoneCode":"+1"},{"name":"Brunei","code":"BN","phoneCode":"+673"},{"name":"Bulgaria","code":"BG","phoneCode":"+359"},{"name":"Burkina Faso","code":"BF","phoneCode":"+226"},{"name":"Burundi","code":"BI","phoneCode":"+257"},{"name":"Cambodia","code":"KH","phoneCode":"+855"},{"name":"Cameroon","code":"CM","phoneCode":"+237"},{"name":"Canada","code":"CA","phoneCode":"+1"},{"name":"Cape Verde","code":"CV","phoneCode":"+238"},{"name":"Caribbean Netherlands","code":"BQ","phoneCode":"+599"},{"name":"Cayman Islands","code":"KY","phoneCode":"+1"},{"name":"Central African Republic","code":"CF","phoneCode":"+236"},{"name":"Chad","code":"TD","phoneCode":"+235"},{"name":"Chile","code":"CL","phoneCode":"+56"},{"name":"China","code":"CN","phoneCode":"+86"},{"name":"Christmas Island","code":"CX","phoneCode":"+61"},{"name":"Cocos (Keeling) Islands","code":"CC","phoneCode":"+61"},{"name":"Colombia","code":"CO","phoneCode":"+57"},{"name":"Comoros","code":"KM","phoneCode":"+269"},{"name":"Congo - Brazzaville","code":"CG","phoneCode":"+242"},{"name":"Congo - Kinshasa","code":"CD","phoneCode":"+243"},{"name":"Cook Islands","code":"CK","phoneCode":"+682"},{"name":"Costa Rica","code":"CR","phoneCode":"+506"},{"name":"C\u00F4te d\u2019Ivoire","code":"CI","phoneCode":"+225"},{"name":"Croatia","code":"HR","phoneCode":"+385"},{"name":"Cuba","code":"CU","phoneCode":"+53"},{"name":"Cura\u00E7ao","code":"CW","phoneCode":"+599"},{"name":"Cyprus","code":"CY","phoneCode":"+357"},{"name":"Czechia","code":"CZ","phoneCode":"+420"},{"name":"Denmark","code":"DK","phoneCode":"+45"},{"name":"Djibouti","code":"DJ","phoneCode":"+253"},{"name":"Dominica","code":"DM","phoneCode":"+1"},{"name":"Dominican Republic","code":"DO","phoneCode":"+1"},{"name":"Ecuador","code":"EC","phoneCode":"+593"},{"name":"Egypt","code":"EG","phoneCode":"+20"},{"name":"El Salvador","code":"SV","phoneCode":"+503"},{"name":"Equatorial Guinea","code":"GQ","phoneCode":"+240"},{"name":"Eritrea","code":"ER","phoneCode":"+291"},{"name":"Estonia","code":"EE","phoneCode":"+372"},{"name":"Eswatini","code":"SZ","phoneCode":"+268"},{"name":"Ethiopia","code":"ET","phoneCode":"+251"},{"name":"Falkland Islands","code":"FK","phoneCode":"+500"},{"name":"Faroe Islands","code":"FO","phoneCode":"+298"},{"name":"Fiji","code":"FJ","phoneCode":"+679"},{"name":"Finland","code":"FI","phoneCode":"+358"},{"name":"France","code":"FR","phoneCode":"+33"},{"name":"French Guiana","code":"GF","phoneCode":"+594"},{"name":"French Polynesia","code":"PF","phoneCode":"+689"},{"name":"Gabon","code":"GA","phoneCode":"+241"},{"name":"Gambia","code":"GM","phoneCode":"+220"},{"name":"Georgia","code":"GE","phoneCode":"+995"},{"name":"Germany","code":"DE","phoneCode":"+49"},{"name":"Ghana","code":"GH","phoneCode":"+233"},{"name":"Gibraltar","code":"GI","phoneCode":"+350"},{"name":"Greece","code":"GR","phoneCode":"+30"},{"name":"Greenland","code":"GL","phoneCode":"+299"},{"name":"Grenada","code":"GD","phoneCode":"+1"},{"name":"Guadeloupe","code":"GP","phoneCode":"+590"},{"name":"Guam","code":"GU","phoneCode":"+1"},{"name":"Guatemala","code":"GT","phoneCode":"+502"},{"name":"Guernsey","code":"GG","phoneCode":"+44"},{"name":"Guinea","code":"GN","phoneCode":"+224"},{"name":"Guinea-Bissau","code":"GW","phoneCode":"+245"},{"name":"Guyana","code":"GY","phoneCode":"+592"},{"name":"Haiti","code":"HT","phoneCode":"+509"},{"name":"Honduras","code":"HN","phoneCode":"+504"},{"name":"Hong Kong SAR China","code":"HK","phoneCode":"+852"},{"name":"Hungary","code":"HU","phoneCode":"+36"},{"name":"Iceland","code":"IS","phoneCode":"+354"},{"name":"India","code":"IN","phoneCode":"+91"},{"name":"Indonesia","code":"ID","phoneCode":"+62"},{"name":"Iran","code":"IR","phoneCode":"+98"},{"name":"Iraq","code":"IQ","phoneCode":"+964"},{"name":"Ireland","code":"IE","phoneCode":"+353"},{"name":"Isle of Man","code":"IM","phoneCode":"+44"},{"name":"Israel","code":"IL","phoneCode":"+972"},{"name":"Italy","code":"IT","phoneCode":"+39"},{"name":"Jamaica","code":"JM","phoneCode":"+1"},{"name":"Japan","code":"JP","phoneCode":"+81"},{"name":"Jersey","code":"JE","phoneCode":"+44"},{"name":"Jordan","code":"JO","phoneCode":"+962"},{"name":"Kazakhstan","code":"KZ","phoneCode":"+7"},{"name":"Kenya","code":"KE","phoneCode":"+254"},{"name":"Kiribati","code":"KI","phoneCode":"+686"},{"name":"Kosovo","code":"XK","phoneCode":"+383"},{"name":"Kuwait","code":"KW","phoneCode":"+965"},{"name":"Kyrgyzstan","code":"KG","phoneCode":"+996"},{"name":"Laos","code":"LA","phoneCode":"+856"},{"name":"Latvia","code":"LV","phoneCode":"+371"},{"name":"Lebanon","code":"LB","phoneCode":"+961"},{"name":"Lesotho","code":"LS","phoneCode":"+266"},{"name":"Liberia","code":"LR","phoneCode":"+231"},{"name":"Libya","code":"LY","phoneCode":"+218"},{"name":"Liechtenstein","code":"LI","phoneCode":"+423"},{"name":"Lithuania","code":"LT","phoneCode":"+370"},{"name":"Luxembourg","code":"LU","phoneCode":"+352"},{"name":"Macao SAR China","code":"MO","phoneCode":"+853"},{"name":"Madagascar","code":"MG","phoneCode":"+261"},{"name":"Malawi","code":"MW","phoneCode":"+265"},{"name":"Malaysia","code":"MY","phoneCode":"+60"},{"name":"Maldives","code":"MV","phoneCode":"+960"},{"name":"Mali","code":"ML","phoneCode":"+223"},{"name":"Malta","code":"MT","phoneCode":"+356"},{"name":"Marshall Islands","code":"MH","phoneCode":"+692"},{"name":"Martinique","code":"MQ","phoneCode":"+596"},{"name":"Mauritania","code":"MR","phoneCode":"+222"},{"name":"Mauritius","code":"MU","phoneCode":"+230"},{"name":"Mayotte","code":"YT","phoneCode":"+262"},{"name":"Mexico","code":"MX","phoneCode":"+52"},{"name":"Micronesia","code":"FM","phoneCode":"+691"},{"name":"Moldova","code":"MD","phoneCode":"+373"},{"name":"Monaco","code":"MC","phoneCode":"+377"},{"name":"Mongolia","code":"MN","phoneCode":"+976"},{"name":"Montenegro","code":"ME","phoneCode":"+382"},{"name":"Montserrat","code":"MS","phoneCode":"+1"},{"name":"Morocco","code":"MA","phoneCode":"+212"},{"name":"Mozambique","code":"MZ","phoneCode":"+258"},{"name":"Myanmar (Burma)","code":"MM","phoneCode":"+95"},{"name":"Namibia","code":"NA","phoneCode":"+264"},{"name":"Nauru","code":"NR","phoneCode":"+674"},{"name":"Nepal","code":"NP","phoneCode":"+977"},{"name":"Netherlands","code":"NL","phoneCode":"+31"},{"name":"New Caledonia","code":"NC","phoneCode":"+687"},{"name":"New Zealand","code":"NZ","phoneCode":"+64"},{"name":"Nicaragua","code":"NI","phoneCode":"+505"},{"name":"Niger","code":"NE","phoneCode":"+227"},{"name":"Nigeria","code":"NG","phoneCode":"+234"},{"name":"Niue","code":"NU","phoneCode":"+683"},{"name":"Norfolk Island","code":"NF","phoneCode":"+672"},{"name":"Northern Mariana Islands","code":"MP","phoneCode":"+1"},{"name":"North Korea","code":"KP","phoneCode":"+850"},{"name":"North Macedonia","code":"MK","phoneCode":"+389"},{"name":"Norway","code":"NO","phoneCode":"+47"},{"name":"Oman","code":"OM","phoneCode":"+968"},{"name":"Pakistan","code":"PK","phoneCode":"+92"},{"name":"Palau","code":"PW","phoneCode":"+680"},{"name":"Palestinian Territories","code":"PS","phoneCode":"+970"},{"name":"Panama","code":"PA","phoneCode":"+507"},{"name":"Papua New Guinea","code":"PG","phoneCode":"+675"},{"name":"Paraguay","code":"PY","phoneCode":"+595"},{"name":"Peru","code":"PE","phoneCode":"+51"},{"name":"Philippines","code":"PH","phoneCode":"+63"},{"name":"Poland","code":"PL","phoneCode":"+48"},{"name":"Portugal","code":"PT","phoneCode":"+351"},{"name":"Puerto Rico","code":"PR","phoneCode":"+1"},{"name":"Qatar","code":"QA","phoneCode":"+974"},{"name":"R\u00E9union","code":"RE","phoneCode":"+262"},{"name":"Romania","code":"RO","phoneCode":"+40"},{"name":"Russia","code":"RU","phoneCode":"+7"},{"name":"Rwanda","code":"RW","phoneCode":"+250"},{"name":"Samoa","code":"WS","phoneCode":"+685"},{"name":"San Marino","code":"SM","phoneCode":"+378"},{"name":"S\u00E3o Tom\u00E9 & Pr\u00EDncipe","code":"ST","phoneCode":"+239"},{"name":"Saudi Arabia","code":"SA","phoneCode":"+966"},{"name":"Senegal","code":"SN","phoneCode":"+221"},{"name":"Serbia","code":"RS","phoneCode":"+381"},{"name":"Seychelles","code":"SC","phoneCode":"+248"},{"name":"Sierra Leone","code":"SL","phoneCode":"+232"},{"name":"Singapore","code":"SG","phoneCode":"+65"},{"name":"Sint Maarten","code":"SX","phoneCode":"+1"},{"name":"Slovakia","code":"SK","phoneCode":"+421"},{"name":"Slovenia","code":"SI","phoneCode":"+386"},{"name":"Solomon Islands","code":"SB","phoneCode":"+677"},{"name":"Somalia","code":"SO","phoneCode":"+252"},{"name":"South Africa","code":"ZA","phoneCode":"+27"},{"name":"South Korea","code":"KR","phoneCode":"+82"},{"name":"South Sudan","code":"SS","phoneCode":"+211"},{"name":"Spain","code":"ES","phoneCode":"+34"},{"name":"Sri Lanka","code":"LK","phoneCode":"+94"},{"name":"St. Barth\u00E9lemy","code":"BL","phoneCode":"+590"},{"name":"St. Helena","code":"SH","phoneCode":"+290"},{"name":"St. Kitts & Nevis","code":"KN","phoneCode":"+1"},{"name":"St. Lucia","code":"LC","phoneCode":"+1"},{"name":"St. Martin","code":"MF","phoneCode":"+590"},{"name":"St. Pierre & Miquelon","code":"PM","phoneCode":"+508"},{"name":"St. Vincent & Grenadines","code":"VC","phoneCode":"+1"},{"name":"Sudan","code":"SD","phoneCode":"+249"},{"name":"Suriname","code":"SR","phoneCode":"+597"},{"name":"Svalbard & Jan Mayen","code":"SJ","phoneCode":"+47"},{"name":"Sweden","code":"SE","phoneCode":"+46"},{"name":"Switzerland","code":"CH","phoneCode":"+41"},{"name":"Syria","code":"SY","phoneCode":"+963"},{"name":"Taiwan","code":"TW","phoneCode":"+886"},{"name":"Tajikistan","code":"TJ","phoneCode":"+992"},{"name":"Tanzania","code":"TZ","phoneCode":"+255"},{"name":"Thailand","code":"TH","phoneCode":"+66"},{"name":"Timor-Leste","code":"TL","phoneCode":"+670"},{"name":"Togo","code":"TG","phoneCode":"+228"},{"name":"Tokelau","code":"TK","phoneCode":"+690"},{"name":"Tonga","code":"TO","phoneCode":"+676"},{"name":"Trinidad & Tobago","code":"TT","phoneCode":"+1"},{"name":"Tristan da Cunha","code":"TA","phoneCode":"+290"},{"name":"Tunisia","code":"TN","phoneCode":"+216"},{"name":"T\u00FCrkiye","code":"TR","phoneCode":"+90"},{"name":"Turkmenistan","code":"TM","phoneCode":"+993"},{"name":"Turks & Caicos Islands","code":"TC","phoneCode":"+1"},{"name":"Tuvalu","code":"TV","phoneCode":"+688"},{"name":"U.S. Virgin Islands","code":"VI","phoneCode":"+1"},{"name":"Uganda","code":"UG","phoneCode":"+256"},{"name":"Ukraine","code":"UA","phoneCode":"+380"},{"name":"United Arab Emirates","code":"AE","phoneCode":"+971"},{"name":"United Kingdom","code":"GB","phoneCode":"+44"},{"name":"United States","code":"US","phoneCode":"+1"},{"name":"Uruguay","code":"UY","phoneCode":"+598"},{"name":"Uzbekistan","code":"UZ","phoneCode":"+998"},{"name":"Vanuatu","code":"VU","phoneCode":"+678"},{"name":"Vatican City","code":"VA","phoneCode":"+39"},{"name":"Venezuela","code":"VE","phoneCode":"+58"},{"name":"Vietnam","code":"VN","phoneCode":"+84"},{"name":"Wallis & Futuna","code":"WF","phoneCode":"+681"},{"name":"Western Sahara","code":"EH","phoneCode":"+212"},{"name":"Yemen","code":"YE","phoneCode":"+967"},{"name":"Zambia","code":"ZM","phoneCode":"+260"},{"name":"Zimbabwe","code":"ZW","phoneCode":"+263"}],"initialPhoneFormat":{"id":0,"type":"PHONE_NUMBER","country":"GB","labelLocale":"en-US","fields":[{"type":"FIELD","label":"1","identifier":"1","length":4,"required":false,"metadata":{}},{"type":"SEPARATOR","label":" ","identifier":"Space","length":0,"required":false,"metadata":{}},{"type":"FIELD","label":"2","identifier":"2","length":16,"required":false,"metadata":{}}]}},"localizedStrings":{"validation":{"noValidSelection":"A valid selection must be made.","invalidUrl":"Must be a valid URL.","stringTooLong":"Value should have a length no longer than {0}.","containsInvalidKey":"{0} contains an invalid key.","invalidTwitterUsername":"Must be a valid Twitter username.","valueOutsideRange":"Value must be in the range {0} to {1}.","invalidPassword":"Passwords should not contain whitespace.","missingRequiredSubfields":"{0} is missing required subfields: {1}","invalidCurrency":"Currency value should be formatted like 1234 or 123.99.","invalidMapSize":"Value should contain exactly {0} elements.","subfieldsRequired":"All fields in {0} are required.","formSubmissionFailed":"Form submission failed. Review the following information: {0}.","invalidCountryCode":"Country code should have an optional plus and up to 4 digits.","invalidDate":"This is not a real date.","required":"{0} is required.","invalidStringLength":"Value should be {0} characters long.","invalidEmail":"Email addresses should follow the format user@domain.com.","invalidListLength":"Value should be {0} elements long.","allEmpty":"Please fill out at least one form field.","missingRequiredQuestion":"Missing a required question.","invalidQuestion":"Contained an invalid question.","captchaFailure":"Captcha validation failed. Please try again.","stringTooShort":"Value should have a length of at least {0}.","invalid":"{0} is not valid.","formErrors":"Form Errors","containsInvalidValue":"{0} contains an invalid value.","invalidUnsignedNumber":"Numbers must contain only digits and no other characters.","invalidName":"Valid names contain only letters, numbers, spaces, ', or - characters."},"submit":"Submit","status":{"title":"{@} Block","learnMore":"Learn more"},"name":{"firstName":"First Name","lastName":"Last Name"},"lightbox":{"openForm":"Open Form"},"likert":{"agree":"Agree","stronglyDisagree":"Strongly Disagree","disagree":"Disagree","stronglyAgree":"Strongly Agree","neutral":"Neutral"},"time":{"am":"AM","second":"Second","pm":"PM","minute":"Minute","amPm":"AM/PM","hour":"Hour"},"notFound":"Form not found.","date":{"yyyy":"YYYY","year":"Year","mm":"MM","day":"Day","month":"Month","dd":"DD"},"phone":{"country":"Country","number":"Number","prefix":"Prefix","areaCode":"Area Code","line":"Line"},"submitError":"Unable to submit form. Please try again later.","address":{"stateProvince":"State/Province","country":"Country","zipPostalCode":"Zip/Postal Code","address2":"Address 2","address1":"Address 1","city":"City"},"email":{"signUp":"Sign up for news and updates"},"cannotSubmitDemoForm":"This is a demo form and cannot be submitted.","required":"(required)","invalidData":"Invalid form data."}}};</script><link rel="stylesheet" type="text/css" href="https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.code/7fe3dcc7-477a-4b2e-b03c-bf731aeb3444_29/website.components.code.styles.css"/><link rel="stylesheet" type="text/css" href="https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.form/727e4778-d924-48fd-95b7-a97ab4b42640_155/website.components.form.styles.css"/><script src="https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.code/7fe3dcc7-477a-4b2e-b03c-bf731aeb3444_29/website.components.code.visitor.js"></script><script src="https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.form/727e4778-d924-48fd-95b7-a97ab4b42640_155/website.components.form.visitor.js"></script><script>Squarespace.load(window);</script>
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://www.tobyord.com/writing?format=rss" />
<script type="application/ld+json">{"url":"https://www.tobyord.com","name":"Toby Ord","description":"<p>An Oxford philosopher working on big picture questions facing humanity this century.</p>","@context":"http://schema.org","@type":"WebSite"}</script><script type="application/ld+json">{"name":"Inference Scaling Reshapes AI Governance \u2014 Toby Ord","url":"https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance","datePublished":"2025-02-12T12:31:07+0000","dateModified":"2025-06-23T11:52:02+0100","headline":"Inference Scaling Reshapes AI Governance","author":"Toby Ord","publisher":{"name":"Toby Ord","logo":{"@type":"ImageObject"},"@context":"http://schema.org","@type":"Organization"},"image":"http://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/t/67add230f85ccb699eec4584/1739444784967/IDA-wide-opaque.png?format=1500w","@context":"http://schema.org","@type":"Article"}</script><link rel="stylesheet" type="text/css" href="https://static1.squarespace.com/static/sitecss/562652dbe4b05bbfdc596fd7/148/5093f261e4b0979eac7cb299/562652dbe4b05bbfdc596fde/3323/site.css"/><!-- MathJax on squarespace courtesy of https://www.hepguy.com/secondary/mathjax-on-squarespace -->

<!-- #1: Load MathJax-->
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- The following scripts only work AFTER the script that loads MathJax -->

<!-- #2: Enable equation numbers via \begin{equation}. -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!-- #3: Enable proper inline equation via $, and also process escape characters (e.g. \$). -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes:true}
});
</script>

<!-- #4: Create a MathJax macro that reset's the equation auto-numbers. 
Thanks to https://github.com/mathjax/MathJax/issues/1294 -->
<script type="text/x-mathjax-config">
MathJax.InputJax.TeX.Definitions.Add({
        macros: {
          setCounter: "setCounter"
        }
      }, null, true);
      MathJax.InputJax.TeX.Parse.Augment({
        setCounter: function(name) {
          var num =  parseInt(this.GetArgument(name));
          MathJax.Extension["TeX/AMSmath"].number = num;
        }
      });
 </script><style>
.date-author, .like, .squarespace-social-buttons, .pagination, .tags-cats{
  display:none !important;
}

</style><script>Static.COOKIE_BANNER_CAPABLE = true;</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-74064079-1"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZjQwMz', true);gtag('config', 'UA-74064079-1');</script><!-- End of Squarespace Headers -->
    <script src="https://static1.squarespace.com/static/ta/5093f258e4b0979eac7cb197/3323/scripts/site-bundle.js" type="text/javascript"></script>
  </head>
  
  <body id="item-67ac8d877fb3420883967ac0" class="header-icons header-dropdown-contact-or-location-none always-show-nav  title-decoration-none  blog-text-width-narrow  show-category-navigation   event-show-past-events event-thumbnails event-thumbnail-size-32-standard event-date-label  event-list-show-cats event-list-date event-list-time event-list-address   event-icalgcal-links  event-excerpts      gallery-design-slideshow aspect-ratio-auto lightbox-style-light gallery-navigation-circles gallery-info-overlay-show-on-hover gallery-aspect-ratio-32-standard gallery-arrow-style-no-background gallery-transitions-scroll  gallery-auto-crop gallery-autoplay gallery-loop product-list-titles-under product-list-alignment-center product-item-size-11-square product-image-auto-crop product-gallery-size-11-square product-gallery-auto-crop show-product-price show-product-item-nav product-social-sharing tweak-v1-related-products-image-aspect-ratio-11-square tweak-v1-related-products-details-alignment-center newsletter-style-dark hide-opentable-icons opentable-style-dark small-button-style-solid small-button-shape-square medium-button-style-solid medium-button-shape-square large-button-style-solid large-button-shape-square image-block-poster-text-alignment-center image-block-card-dynamic-font-sizing image-block-card-content-position-center image-block-card-text-alignment-left image-block-overlap-dynamic-font-sizing image-block-overlap-content-position-center image-block-overlap-text-alignment-left image-block-collage-dynamic-font-sizing image-block-collage-content-position-top image-block-collage-text-alignment-left image-block-stack-dynamic-font-sizing image-block-stack-text-alignment-left button-style-solid button-corner-style-square tweak-product-quick-view-button-style-floating tweak-product-quick-view-button-position-bottom tweak-product-quick-view-lightbox-excerpt-display-truncate tweak-product-quick-view-lightbox-show-arrows tweak-product-quick-view-lightbox-show-close-button tweak-product-quick-view-lightbox-controls-weight-light native-currency-code-usd collection-605a132f7a36283cfa0c4af9 view-item collection-type-blog collection-layout-default mobile-style-available has-main-image has-description">

    <div class="contact-wrapper">
      <div class="contact-inner-wrapper">
        <div class="sqs-layout sqs-grid-12 columns-12 sqs-locked-layout" data-layout-label="Header Contact Info Block" data-type="block-field" data-updated-on="1413387585375" id="contactBlock"><div class="row sqs-row"><div class="col sqs-col-6 span-6"><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-59fffaa48d3b263d8f7e"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <h3>Contact Us</h3><p>Use the form on the right to contact us.</p><p>You can edit the text in this area, and change where the contact form on the right submits to, by entering edit mode using the modes on the bottom right.&nbsp;</p>
</div>






















</div></div></div><div class="col sqs-col-6 span-6"><div class="sqs-block website-component-block sqs-block-website-component sqs-block-form form-block" data-block-css="[&quot;https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.form/727e4778-d924-48fd-95b7-a97ab4b42640_155/website.components.form.styles.css&quot;]" data-block-scripts="[&quot;https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.form/727e4778-d924-48fd-95b7-a97ab4b42640_155/website.components.form.visitor.js&quot;]" data-block-type="1337" data-definition-name="website.components.form" data-sqsp-block="form" id="block-99eb5dae31962e780f96"><div class="sqs-block-content"><div class="sqs-site-style-form">
  
  <script type="application/json" id="form-context-562652dbe4b05bbfdc596fdf" class="sqs-form-block-context">
  {"secureUrl":null,"collectionId":null,"formFieldFormats":null,"useFormsJs":null,"formId":null,"formName":null,"formFields":null,"formSubmitButtonText":null,"formSubmissionMessage":null,"successRedirect":null,"disclaimerMessage":null,"captchaEnabled":null,"captchaTheme":null,"captchaAlignment":null}
  </script>
  
  <script type="application/json" id="form-block-design-fields-562652dbe4b05bbfdc596fdf" class="sqs-form-block-design-fields">
  {"buttonAlignment":"left","buttonVariant":null,"firstFieldHighlightType":null,"submissionTextAlignment":null,"submissionVerticalAlignment":null,"submissionAnimation":null}
  </script>
  <script type="application/json" id="form-context-localized-strings-562652dbe4b05bbfdc596fdf" class="sqs-form-block-localized-strings">
  {"time":{"amPm":"AM/PM","pm":"PM","minute":"Minute","second":"Second","hour":"Hour","am":"AM"},"validation":{"noValidSelection":"A valid selection must be made.","invalidPassword":"Passwords should not contain whitespace.","formErrors":"Form Errors","invalidStringLength":"Value should be {0} characters long.","invalidUrl":"Must be a valid URL.","invalidDate":"This is not a real date.","invalidMapSize":"Value should contain exactly {0} elements.","invalidListLength":"Value should be {0} elements long.","invalidEmail":"Email addresses should follow the format user@domain.com.","stringTooLong":"Value should have a length no longer than {0}.","required":"{0} is required.","containsInvalidKey":"{0} contains an invalid key.","invalidQuestion":"Contained an invalid question.","invalid":"{0} is not valid.","formSubmissionFailed":"Form submission failed. Review the following information: {0}.","invalidUnsignedNumber":"Numbers must contain only digits and no other characters.","invalidCountryCode":"Country code should have an optional plus and up to 4 digits.","stringTooShort":"Value should have a length of at least {0}.","subfieldsRequired":"All fields in {0} are required.","invalidTwitterUsername":"Must be a valid X username.","containsInvalidValue":"{0} contains an invalid value.","missingRequiredQuestion":"Missing a required question.","missingRequiredSubfields":"{0} is missing required subfields: {1}","allEmpty":"Please fill out at least one form field.","captchaFailure":"Captcha validation failed. Please try again.","valueOutsideRange":"Value must be in the range {0} to {1}.","invalidName":"Valid names contain only letters, numbers, spaces, ', or - characters.","invalidCurrency":"Currency value should be formatted like 1234 or 123.99."},"date":{"year":"Year","month":"Month","dd":"DD","day":"Day","mm":"MM","yyyy":"YYYY"},"likert":{"agree":"Agree","neutral":"Neutral","stronglyAgree":"Strongly Agree","disagree":"Disagree","stronglyDisagree":"Strongly Disagree"},"name":{"lastName":"Last Name","firstName":"First Name"},"required":"(required)","email":{"required":"(required)","signUp":"Sign up for news and updates"},"phone":{"areaCode":"Area Code","line":"Line","prefix":"Prefix","country":"Country","number":"Number"},"lightbox":{"openForm":"Open Form"},"address":{"zipPostalCode":"Zip/Postal Code","city":"City","address2":"Address 2","address1":"Address 1","stateProvince":"State/Province","country":"Country"},"notFound":"Form not found.","submit":"Submit","submitError":"Unable to submit form. Please try again later.","cannotSubmitDemoForm":"This is a demo form and cannot be submitted.","invalidData":"Invalid form data."}
  </script>
  <div id="form-submission-html-562652dbe4b05bbfdc596fdf" class="sqs-form-block-submission-html" data-submission-html=""></div>

  <div class="form-wrapper"
    
  >
    
  </div>
</div></div></div></div></div></div>
      </div>
    </div>

    <div id="contactInfo">
      <div class="contact-inner-wrapper contact-info-inner-wrapper">

        <div class="map-image"></div>

        <div class="address-phone-email">
              <p>123 Street Avenue, City Town, 99999</p>
              <p>(123) 555-6789</p>
              <p>email@address.com</p>
              <p>&nbsp;</p>
              <p>You can set your address, phone number, email and site description in the settings tab.<br />Link to <a href="/read-me">read me page</a> with more information.</p>
            </div>

      </div>
    </div>
    <nav id="mobile-navigation" class="mobile-nav">
      
<div class="nav-wrapper" data-content-field="navigation-mobileNav">
  <ul>
  
  
    
    <li class=""><a href="/">About</a></li>
    
    
  
  
  
    
    <li class=""><a href="/book">Book</a></li>
    
    
  
  
  
    
    <li class=""><a href="/research">Research</a></li>
    
    
  
  
  
    
    <li class=""><a href="/writings">Writing</a></li>
    
    
  
  
  
    
    <li class=""><a href="/projects">Projects</a></li>
    
    
  
  
  
    
    <li class=""><a href="/media">Media</a></li>
    
    
  
  
  
    
    <li class=""><a href="/earth">Earth</a></li>
    
    
  
  
  
  </ul>
</div>


    </nav>
    <div class="opacity-overlay"></div>
    <div class="outer-wrapper">
      <div class="wrapper cf">
        <div id="mobile-header">
          <a class="icon-menu" id="mobileMenu"></a><!-- comment the space between elements because science
          --><div class="site-title-wrapper">
            <h1 data-content-field="site-title" class="site-title">
              <a href="/" class="home-link">Toby Ord</a>
            </h1>
          </div><!-- comment the space between elements because science
          --><div class="info-email-wrapper">
            <a id="info-mobile" class="icon-info"><span class="icon-text">Info</span></a>
            <div id="email-mobile">
              <a class="icon-email"><span class="icon-text">Email</span></a>
            </div>
            <div id="search-mobile">
              <a class="icon-search" href="/search" data-source="template"><span class="icon-text">Search</span></a>
            </div>
          </div>
        </div>
        <header id="header">

          <!--MAIN NAVIGATION-->
          <nav id="main-navigation" class="main-nav">
            <div class="info-title-email-wrapper">

              <a class="icon-menu" id="desktopMenu"><span class="icon-text">Menu</span></a>

              
              <div class="site-title-wrapper">
                <h1 data-content-field="site-title" class="site-title">
                    
                      <a href="/" class="home-link">Toby Ord</a>
                    
                </h1>
              </div>
              

              <a id="info" class="icon-info"><span class="icon-text">Info</span></a>
              <div id="email">
                <a class="icon-email"><span class="icon-text">Email</span></a>
              </div>
              <div id="headerSearch">
                <a class="icon-search" href="/search" data-source="template"><span class="icon-text">Search</span></a>
              </div>
            </div>
            
<div class="nav-wrapper" data-content-field="navigation-mainNav">
  <ul class="cf">
  
    

    
    <li class=""><a href="/">About</a></li>
    

    
    
  
    

    
    <li class=""><a href="/book">Book</a></li>
    

    
    
  
    

    
    <li class=""><a href="/research">Research</a></li>
    

    
    
  
    

    
    <li class=""><a href="/writings">Writing</a></li>
    

    
    
  
    

    
    <li class=""><a href="/projects">Projects</a></li>
    

    
    
  
    

    
    <li class=""><a href="/media">Media</a></li>
    

    
    
  
    

    
    <li class=""><a href="/earth">Earth</a></li>
    

    
    
  

  
  </ul>
</div>


          </nav>
        </header>
        <!--CONTENT INJECTION POINT-->

        <section id="content">
          
            
              
            
          
        

        

        <div class="content-inner-wrapper">
          <div class="collection-title-desc" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
            <div class="collection-title-basic"><h1 class="page-title">Writing</h1></div>
            
          </div>
            <div class="main-content-wrapper" data-content-field="main-content" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
              <div class="post-wrapper">
<!--WRAPPER-->
<section class="body">  
  <article id="post-67ac8d877fb3420883967ac0" class="hentry author-toby-ord post-type-text" data-item-id="67ac8d877fb3420883967ac0">
    
    <!--POST TILE-->

    <h1 class="title">
      
        <a href="/writing/inference-scaling-reshapes-ai-governance">Inference Scaling Reshapes AI Governance</a>
      
    </h1>

    <h2 class="date-author"><time datetime="2025-02-12">February 12, 2025</time><a href="/writing?author=562a0c98e4b04076bfdb0dd2" class="author">Toby Ord</a></h2>

    <!--MAIN CONTENT-->

    <div class="sqs-layout sqs-grid-12 columns-12" data-layout-label="Post Body" data-type="item" data-updated-on="1739361788691" id="item-67ac8d877fb3420883967ac0"><div class="row sqs-row"><div class="col sqs-col-12 span-12"><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-810f080dba670bcca8af"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <p style="text-align:justify;white-space: normal !important;margin-left:40px;white-space:pre-wrap;" class=""><em>The shift from scaling up the pre-training compute of AI systems to scaling up their inference compute may have profound effects on AI governance. The nature of these effects depends crucially on whether this new inference compute will primarily be used during external deployment or as part of a more complex training programme within the lab. Rapid scaling of inference-at-deployment would: lower the importance of open-weight models (and of securing the weights of closed models), reduce the impact of the first human-level models, change the business model for frontier AI, reduce the need for power-intense data centres, and derail the current paradigm of AI governance via training compute thresholds. Rapid scaling of inference-during-training would have more ambiguous effects that range from a revitalisation of pre-training scaling to a form of recursive self-improvement via iterated distillation and amplification.</em></p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>The end of an era — for both training and governance</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The intense year-on-year scaling up of AI training runs has been one of the most dramatic and stable markers of the Large Language Model era. Indeed it had been widely taken to be a permanent fixture of the AI landscape and the basis of many approaches to AI governance.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But <a href="https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/"><span style="text-decoration:underline">recent</span></a> <a href="https://www.deeplearning.ai/the-batch/ai-giants-rethink-model-training-strategy-as-scaling-laws-break-down/"><span style="text-decoration:underline">reports</span></a> from unnamed employees at the leading labs suggest that their attempts to scale up pre-training substantially beyond the size of GPT-4 have led to only modest gains which are insufficient to justify continuing such scaling and perhaps even insufficient to warrant public deployment of those models. A possible reason is that they are running out of high-quality training data. While the scaling laws might still be operating (given sufficient compute and data, the models would keep improving), the ability to harness them through rapid scaling of pre-training may not. What was taken to be a fixture may instead have been just one important era in the history of AI development; an era which is now coming to a close.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Just before the reports of difficulties scaling pre-training, OpenAI <a href="https://openai.com/index/learning-to-reason-with-llms/"><span style="text-decoration:underline">announced</span></a> their breakthrough reasoning model, o1. Their announcement came with a chart showing how its performance on a difficult mathematics benchmark could be increased via scaling compute dedicated to post-training reinforcement learning (to improve the overall performance of the model); or by scaling the inference compute used on the current task.</p>
</div>




















  
  



</div></div><div class="sqs-block image-block sqs-block-image" data-block-type="5" id="block-yui_3_17_2_1_1739361802903_10832"><div class="sqs-block-content">










































  

    
  
    <div
        class="
          image-block-outer-wrapper
          layout-caption-hidden
          design-layout-inline
          combination-animation-none
          individual-animation-none
          individual-text-animation-none
        "
        data-test="image-block-inline-outer-wrapper"
    >

      

      
        <figure
            class="
              sqs-block-image-figure
              intrinsic
            "
            style="max-width:1980px;"
        >
          
        
        

        
          
            
          <div
              
              
              class="image-block-wrapper"
              data-animation-role="image"
              
  

          >
            <div class="sqs-image-shape-container-element
              
          
        
              has-aspect-ratio
            " style="
                position: relative;
                
                  padding-bottom:56.212120056152344%;
                
                overflow: hidden;-webkit-mask-image: -webkit-radial-gradient(white, black);
              "
              >
                
                
                
                
                
                
                
                <img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png" data-image-dimensions="1980x1113" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block"  src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png" width="1980" height="1113" alt="" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" style="display:block;object-fit: cover; width: 100%; height: 100%; object-position: 50% 50%" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0a2e094a-7089-4e66-b955-cf7e2b7271b2/o1+performance.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </div>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1739361802903_11372"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This has led to intense speculation that the previous era of scaling pre-training compute could be followed by an era of scaling up inference-compute. In this essay, I explore the implications of this possibility for AI governance.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">In some ways a move to scaling of inference compute may be a continuation of the previous paradigm (as lab leaders have been suggesting$^1$). For example, work on the trade-off between pre-training compute and inference compute suggests that (on the current margins) increasing inference compute on the task at hand by 1 order of magnitude often improves performance as much as increasing pre-training compute by <a href="https://epoch.ai/blog/trading-off-compute-in-training-and-inference">0.5 to 1 orders of magnitude</a>. So we may be tempted to see it simply as an implementation detail in the bigger story of scaling up compute in general.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But a closer look suggests that may be a mistake. There are a number of key differences between scaling pre-training and scaling inference — both for the labs and for AI governance.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">I shall argue that many ideas in AI governance will need either an adjustment or an overhaul. Those of us in the field need to look back at the long list of ideas we work with and see how this affects each one.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">There is a lot of uncertainty about what is changing and what will come next. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">One question is the rate at which pre-training will continue to scale. It may be that pre-training has topped out at a GPT-4 scale model, or it may continue increasing, but at a slower rate than before. <a href="https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year"><span style="text-decoration:underline">Epoch AI suggests</span></a> the compute used in LLM pre-training has been growing at about 5x per year from 2020 to 2024. It seems like that rate has now fallen, but it is not yet clear if it has gone to zero (with AI progress coming from things other than pre-training compute) or to some fraction of its previous rate.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">A second — and ultimately more important — question concerns the nature of inference-scaling. We can view the current AI pipeline as pre-training (such as via next-token prediction), followed by post-training (such as RLHF or RLAIF), followed by deploying the trained model on a vast number of different tasks (such as through a chat interface or API calls). </p>
</div>




















  
  



</div></div><div class="sqs-block image-block sqs-block-image" data-block-type="5" id="block-yui_3_17_2_1_1739361802903_14963"><div class="sqs-block-content">










































  

    
  
    <div
        class="
          image-block-outer-wrapper
          layout-caption-hidden
          design-layout-inline
          combination-animation-none
          individual-animation-none
          individual-text-animation-none
        "
        data-test="image-block-inline-outer-wrapper"
    >

      

      
        <figure
            class="
              sqs-block-image-figure
              intrinsic
            "
            style="max-width:1280px;"
        >
          
        
        

        
          
            
          <div
              
              
              class="image-block-wrapper"
              data-animation-role="image"
              
  

          >
            <div class="sqs-image-shape-container-element
              
          
        
              has-aspect-ratio
            " style="
                position: relative;
                
                  padding-bottom:16.640625%;
                
                overflow: hidden;-webkit-mask-image: -webkit-radial-gradient(white, black);
              "
              >
                
                
                
                
                
                
                
                <img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png" data-image-dimensions="1280x213" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block"  src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png" width="1280" height="213" alt="" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" style="display:block;object-fit: cover; width: 100%; height: 100%; object-position: 50% 50%" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/92956b40-e44e-4678-b56e-f58110b5a7fa/Training+Stages.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </div>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1739361802903_15280"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The second question is whether the scaled-up inference compute will primarily be spent during deployment (like in o1 and R1) or as part of a larger and more complex post-training process (like the suggestions that OpenAI may have used trained o3 via many runs of o1). Each of these possibilities has important — but different — implications for AI governance.</p><h2 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Scaling inference-at-deployment</strong></h2><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Let’s first consider the scenario where for the coming years, the lion’s share of compute scaling goes into scaling up the inference compute used at deployment. In this scenario, the pre-trained system is either stuck at GPT-4 level or only slowly progressing beyond that, while new capabilities are being rapidly unlocked via more and more inference compute. Some of this may be being spent in post-training as the system learns how to productively reason for longer times (e.g. the reinforcement learning in the left-hand chart of OpenAI’s o1 announcement), but for this scenario, we are supposing that this one-off cost is comparatively small and that the main thing being scaled is the deployment compute.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">In this scenario, we may be able to use rules of thumb such as</p><p style="text-align:justify;white-space: normal !important;margin-left:40px;white-space:pre-wrap;" class="">Effective orders of magnitude = OOMs of pre-training + 0.7 × OOMs of inference</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">to estimate the capabilities of an inference-scaled model in terms of the familiar yardstick of pure pre-training. But overreliance on such formulas could obscure key changes in the new scaling paradigm — changes that stem from the way the benefits of inference-at-deployment depend upon the task at hand, the way the amount of inference can be tuned to the task, and the way the costs shift from training time to deployment time.&nbsp;&nbsp; </p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Reducing the number of simultaneously served copies of each new model</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">It currently takes a vast number of chips to train a frontier model. Once the model is trained, those chips can be used for inference to deploy a large number of simultaneous copies of that model. Dario Amodei of Anthropic <a href="https://darioamodei.com/machines-of-loving-grace"><span style="text-decoration:underline">estimates</span></a> this to be ‘millions’ of copies. This number of copies is a key parameter for AI governance as it affects the size of the immediate impact on the world the day the new model is ready. A shift to scaling inference-at-deployment would lower this number. e.g. if inference-at-deployment is scaled by two orders of magnitude, then this key parameter goes down by a factor of 100, and the new model can only be immediately deployed in 1% as many tasks as it would be if it had been scaled by pre-training compute.$^2$</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Increasing the price of first human-level AGI systems</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">A related parameter is how expensive the first ‘human-level’ AI systems will be to run. By previous scaling trends we might expect the first such systems to cost much less than human labour, meaning that they could be immediately deployed at a great profit, which could be ploughed back into renting chips to run more copies of them in an escalating feedback loop. But each additional order of magnitude that goes to inference-at-deployment may increase the cost of using these systems by up to an order of magnitude. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This will blunt the immediate impact of reaching this threshold and may even be enough such that there is an initial period where we first have access to ‘human-level’ AGI systems at more than the cost of equivalent human labour. If so, such systems could be available to study (for safety work) or demonstrate (before the world’s leaders) before they have transformative effects on society.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Obviously the fact that AI is already much better than humans at some tasks while much worse at others complicates this idea of reaching ‘human-level’, but I believe it is still a useful lens. For example, you can ask whether the first systems that can perform a particular job better than humans will cost more or less than human wages for that job.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Reducing the value of securing model weights</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Suppose that for frontier models, training compute plateaus at something like the GPT-4 level while inference-at-deployment scales by a factor of 100. Then the value of stealing model weights hasn’t increased over time — it is just the value of not having to train a GPT-4 level model (which has been decreasing over time by <a href="https://epoch.ai/trends"><span style="text-decoration:underline">about 4x</span></a> per year due to algorithmic efficiency improvements and Moore’s law). And even if the weights were stolen, the thief would still have to pay the high inference-at-deployment costs. If they intend to use the model at anything like the scale current leading models are used, these would be the lion’s share of the total costs and much higher than the training costs of the model they stole.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Reducing the benefits and risks of open-weight models</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This also affects both the benefits and drawbacks of open-weight models. If open-weight models require vast amounts of inference-at-deployment from their users, then they are much less attractive to those users than models of equivalent capability that were entirely pre-trained (since then the model trainer has paid those costs for you). So open-weight models could become much less valuable for their users and also less dangerous in terms of proliferation of dangerous capabilities. They would become less strategically important overall.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Unequal performance for different tasks and for different users</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Scaling inference-at-deployment helps with some kinds of tasks much more than others. It helps most with tasks where the solution is objectively verifiable, such as certain kinds of maths and programming tasks. It can also be useful for tasks involving many steps. Two good heuristics for the tasks that benefit from inference scaling are:</p><ol data-rte-list="default"><li><p class="" style="white-space:pre-wrap;">tasks that benefit from System 2–type thinking (methodical reasoning) when performed by humans,</p></li><li><p class="" style="white-space:pre-wrap;">tasks that typically take humans a long time (as this shows these tasks can benefit from a lot of thinking before diminishing marginal returns kick in).</p></li></ol><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Because some tasks benefit more from additional inference than others, it is possible to tailor the amount of inference compute to the task, spending 1,000x the normal amount for a hard, deep maths problem, while just spending 1x on problems that are more intuitive. This kind of tailoring isn’t possible with pre-training scaling, where scaling up by 10x increases the costs for everything.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">A related change is that users with more money will likely be able to convert that into better answers. We’ve already seen this start to happen at OpenAI (the first frontier company to allow access to a model that scales inference-at-deployment). They now charge 10x as much for access to the version using the most inference-compute. We’d become accustomed to a dollar a day getting everyone the same quality of AI assistance. It was as Andy Warhol said about Coca Cola:</p><p style="text-align:justify;white-space: normal !important;margin-left:40px;white-space:pre-wrap;" class="">‘What’s great about this country is that America started the tradition where the richest consumers buy essentially the same things as the poorest. … the President drinks Coca Cola, Liz Taylor drinks Coca Cola, and just think, you can drink Coca Cola, too. A coke is a coke and no amount of money can get you a better coke than the one the bum on the corner is drinking.’</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But scaling inference-at-deployment ends that.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Changing the business model and industry structure</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The LLM business model has had a lot in common with software: big upfront development costs and then comparatively low marginal costs per additional customer. Having a marginal cost per extra user that is lower than the average cost per user encourages economies of scale where each company is incentivised to set low prices to acquire a <em>lot</em> of customers, which in turn tends to create an industry with only a handful of players. But if the next two orders of magnitude of compute scale-up go into inference-at-deployment instead of into pre-training, then this would change, upsetting the existing business model and perhaps allowing more room for smaller players in the industry.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Reducing the need for monolithic data centres</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">While training compute benefits greatly from being localised in the same data centre, inference-at-deployment can be much more easily spread between different locations. Thus if inference-at-deployment is being scaled by several orders of magnitude, it could avoid current bottlenecks concerning single large data centres, such as the need for a large amount of electrical power draw in a single place (which has started to require its own large powerplant). So if one hoped for the government to be able to exert some control over AI labs via the carrot of accelerated power plant approvals, inference-at-deployment may change that. And it will make it harder for governments to keep track of all the frontier models being trained by tracking the largest datacentres.</p><h3 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Breaking the strategy of AI governance via compute thresholds</strong></h3><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class=""><a href="https://arxiv.org/pdf/2405.10799">Many AI governance frameworks</a> are based around regulating only those models above a certain threshold of training compute. For example, the EU AI Act uses 10$^{25}$ FLOP while the US executive order uses 10$^{26}$ FLOP. This allows them to draw a line around the few potentially dangerous systems without needing to regulate the great majority of AI models. But if capabilities can be increased via scaling inference-at-deployment then a model whose training compute was below these thresholds might be amplified to become as powerful as those above them. For example, a model trained with 10$^{24}$ FLOP might have its inference scaled up by 4 OOM and perform at the level of a model trained with 10$^{27}$ FLOP. This threatens to break this entire approach of training-compute thresholds.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">At first the threat might be that someone scales up inference-at-deployment by a very large factor for a small number of important tasks. If the inference scale-up is only happening on a small fraction of all tasks the model is deployed on, one could use a very high scale-up factor (such as 100,000x) and suddenly operate at the level of a new tier of model. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The main limitation on this at the moment is that many current techniques for inference scaling seem to hit plateaus that can’t be exceeded by any level of inference scale-up. Exceeding these plateaus requires substantial research time by AI scientists and engineers, such that if someone tried to use a GPT-4 level model with 100,000x the inference compute, it may not be able to make good use of most of that compute. However, labs are developing better ways to use large multipliers of inference compute before reaching performance plateaus and this work is proceeding very quickly. For example, OpenAI demonstrated their o3 model making use of 10,000x as much compute as their smallest reasoning model, o1-mini, and so presumably an even larger factor above their base model, GPT-4o.</p>
</div>




















  
  



</div></div><div class="sqs-block image-block sqs-block-image" data-block-type="5" id="block-yui_3_17_2_1_1739361802903_72257"><div class="sqs-block-content">










































  

    
  
    <div
        class="
          image-block-outer-wrapper
          layout-caption-below
          design-layout-inline
          combination-animation-none
          individual-animation-none
          individual-text-animation-none
        "
        data-test="image-block-inline-outer-wrapper"
    >

      

      
        <figure
            class="
              sqs-block-image-figure
              intrinsic
            "
            style="max-width:1200px;"
        >
          
        
        

        
          
            
          <div
              
              
              class="image-block-wrapper"
              data-animation-role="image"
              
  

          >
            <div class="sqs-image-shape-container-element
              
          
        
              has-aspect-ratio
            " style="
                position: relative;
                
                  padding-bottom:56.25%;
                
                overflow: hidden;-webkit-mask-image: -webkit-radial-gradient(white, black);
              "
              >
                
                
                
                
                
                
                
                <img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg" data-image-dimensions="1200x675" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block"  src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg" width="1200" height="675" alt="" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" style="display:block;object-fit: cover; width: 100%; height: 100%; object-position: 50% 50%" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/318d5297-2724-4ed3-9d89-c45336f6c79c/o3+performance+on+ARC-AGI.jpeg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </div>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1739361802903_72767"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Leading labs have also been scaling their data centres and improving algorithmic efficiency such that they may already have 100x the effective-compute of the first data centres capable of serving GPT-4 to customers. This would allow more than just a few people to use versions with greatly scaled-up inference-at-deployment. For example, OpenAI’s recently launched deep research model (based on o3) may well exceed the performance of a system pre-trained on 10$^{26}$ FLOP, even if it is technically below that threshold.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">While one could try to change the governance threshold to incorporate the inference-at-deployment as well as the pre-training compute, this would face serious problems. The current framework aims to separate AI systems that could be dangerous from those that can’t be. It aims to regulate dangerous objects, not dangerous uses of objects. But a revised threshold would depend not just on the model but on how you are using it, which would be a different and more challenging kind of governance threshold.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Perhaps one way to save the compute thresholds is to say that they cover both systems above 10$^{26}$ FLOP of pre-training and systems above some smaller threshold (e.g. 10$^{24}$ FLOP of pre-training) that have had post-training to allow themselves to benefit from high inference-at-deployment. But this still suffers from increased complexity and fewer bright lines. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Overall, scaled up inference-at-deployment looks like a big challenge for governance via compute thresholds.</p><h2 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Scaling inference-during-training</strong></h2><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">AI labs may also be able to reap tremendous benefit from these inference-scaled models by using them as part of the training process. If so, the large scale-up of compute resources could go into post-training rather than deployment. This would have very different implications for AI governance.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">In this section, we’ll focus on the implications of a pure strategy of using inference-scaling <em>only</em> during the training process. This will clarify what it contributes to the overall picture of AI governance, though realistically we will see inference-scaling in both training and deployment.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">An obvious approach to scaling inference-during-training is to use an inference-scaled model to generate large amounts of high-quality synthetic data on which to pre-train a new base model. This would make sense if the challenges in scaling up pre-training beyond GPT-4 are due to running out of high-quality training data. For example, <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.391.24.pdf"><span style="text-decoration:underline">court documents</span></a>&nbsp; have revealed that Meta’s Llama3 team decided to train on an illegal Russian repository of copyrighted books, LibGen, because they were unable to reach GPT-4 level without it:</p><p style="text-align:justify;white-space: normal !important;margin-left:40px;white-space:pre-wrap;" class="">‘Libgen is essential to meet SOTA [state-of-the-art] numbers, across all categories, and it is known that OpenAI and Mistral are using the library for their models (through word of mouth).’</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This strongly suggests that even though there are still many more unused tokens on the indexed web (about <a href="https://epoch.ai/blog/can-ai-scaling-continue-through-2030"><span style="text-decoration:underline">30x as many</span></a> as are used in GPT-4 level pre-training), performance is being limited by lack of high-quality tokens. There have already been attempts to supplement the training data with synthetic data (data produced by an LLM), but if the issue is more about quality than raw quantity, then they need the best synthetic data they can get. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Inference-scaling can help with this by boosting the capability of the model producing the synthetic data. One way to do this is via domains such as mathematics or programming where one can tell whether a generated solution is correct and how efficient it is. The training programme could involve generating lots of proofs and computer programs using advanced reasoning models until it finds high quality solutions, and then add those to the stock of data that goes into pre-training the next base model. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This access to ground truth in mathematical disciplines is particularly important for getting the right training signal. But even for domains that are less black and white, it may be possible to trade extra inference compute for better synthetic data. For example, one could generate many essays, run several rounds of editing on them, and then assess them for originality, importance of insight, and lack of detectable errors, putting only those of the highest quality into the stock of synthetic data.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Relatedly, one could apply this technique to the stock of human-generated training data, assessing each document in the training data and discarding those that are below-average in quality. This could either improve the average quality of the data they already use or make some fraction of the unused sources of data usable.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">On its own, this approach of scaling inference-during-training to produce synthetic data for pre-training is not so interesting from an AI governance perspective. Its main direct effect is to allow the scaling of pre-training compute to recommence, breathing new life into the existing scaling paradigm.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But there is a modification of this approach that may have the potential to lead to explosive growth in capabilities. The idea is to rapidly improve a model’s abilities by amplifying its abilities (through inference scaling) then distilling those into a new model, and repeating this process many times. This idea is what powered the advanced self-play in DeepMind’s <a href="https://www.nature.com/articles/nature24270">AlphaGo Zero</a>, and was also independently discovered by <a href="https://arxiv.org/abs/1705.08439">Anthony et al.</a>&nbsp;and, in the context of AI-safety, by <a href="https://ai-alignment.com/benign-model-free-rl-4aae8c97e385">Christiano</a>. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">In the case of AlphaGo Zero, you start with a base model, $M_0$, that takes a representation of the Go board and produces two outputs: a probability distribution over the available moves (representing the chance a skilled player would choose them) and a probability representing the chance the active player will eventually win the game.$^3$ This model will act as an intuitive ‘System 1’ approach to game playing, with no explicit search. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The training technique then plays 25,000 games of Go between two copies of $M_0$ amplified by a probabilistic technique for searching through the tree of moves and countermoves called Monte Carlo Tree Search (MCTS). That is, both players use MCTS with $M_0$ guiding the search by using its estimates of likely moves and position strength as a prior. By repeatedly calling $M_0$ in the search (thousands of times), we get a form of inference-scaling which amplifies the power of this model. We could think of it as taking the raw System 1 intuitions of the base model and embedding them in a System 2 reasoning process which thinks many moves ahead. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">This amplified model is better than the base model at predicting the move most likely to win in each situation, but it is also much more costly. So we train a new model, $M_1$, to predict the outputs of $M_0$ + search. Following Christiano, I shall call this step distillation, though in the case of AlphaGo Zero, $M_1$ was simply $M_0$ with an additional stage of training. This trained its move predictions to be closer to the probability distribution over moves that $M_0$ + search gives and trained its board evaluations to be closer to the final outcome of the self-played games. While $M_1$ won’t be quite as good at Go as the amplified version of $M_0$, it is better than $M_0$ alone. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But why stop there? We can repeat this process, amplifying $M_1$ through inference-scaling by using it to guide the search process, producing a level of play beyond any seen so far ($M_1$ + search). This then gets distilled into a new model, $M_2$, and we proceed onwards and upwards, climbing higher and higher along the ladder of Go-playing performance.</p>
</div>




















  
  



</div></div><div class="sqs-block image-block sqs-block-image" data-block-type="5" id="block-yui_3_17_2_1_1739362831212_16954"><div class="sqs-block-content">










































  

    
  
    <div
        class="
          image-block-outer-wrapper
          layout-caption-hidden
          design-layout-inline
          combination-animation-none
          individual-animation-none
          individual-text-animation-none
        "
        data-test="image-block-inline-outer-wrapper"
    >

      

      
        <figure
            class="
              sqs-block-image-figure
              intrinsic
            "
            style="max-width:4003px;"
        >
          
        
        

        
          
            
          <div
              
              
              class="image-block-wrapper"
              data-animation-role="image"
              
  

          >
            <div class="sqs-image-shape-container-element
              
          
        
              has-aspect-ratio
            " style="
                position: relative;
                
                  padding-bottom:75.99300384521484%;
                
                overflow: hidden;-webkit-mask-image: -webkit-radial-gradient(white, black);
              "
              >
                
                
                
                
                
                
                
                <img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png" data-image-dimensions="4003x3042" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block"  src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png" width="4003" height="3042" alt="" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" style="display:block;object-fit: cover; width: 100%; height: 100%; object-position: 50% 50%" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/fbbb5f48-c287-47d6-bca8-0c5c947154a6/IDA-wide.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </div>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" data-sqsp-block="text" id="block-yui_3_17_2_1_1739362831212_17270"><div class="sqs-block-content">

<div class="sqs-html-content" data-sqsp-text-block-content>
  <p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">After just 36 hours the best model (with search) had exceeded the ability of AlphaGo Lee (the version that beat world-champion Lee Sedol) which had been trained for months but lacked some innovations including this structure of iterated amplification and distillation. Within 72 hours AlphaGo Zero was able to defeat AlphaGo Lee by 100 games to zero. And after 40 days of training (and 29 million games of self-play$^4$) it reached its performance plateau, $M_{max}$, where the unamplified model could no longer meaningfully improve its predictions of the amplified model. At this point the amplified version had an estimated Elo rating of 5,185 — far beyond the 3,739 of AlphaGo Lee or the low 3,000s of the world’s best human players. Even when the final model was used without any search process (i.e. without any scaling of inference-at-deployment), it achieved a rating of 3,055 — roughly at the level of a human pro player despite playing from pure ‘intuition’ with no explicit reasoning.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">It may be possible to use such a process of iterated distillation and amplification in the training of LLMs. The idea would be to take a model such as GPT-4o (which applied a vast amount of pre-training to provide it with a powerful System 1) and use it as the starting model, $M_0$.$^5$<span style="text-decoration:underline">&nbsp;</span>Then amplify it via inference scaling into a model that uses a vast number of calls to $M_0$ to simulate System 2–type internal reasoning before returning its final answer (as o1 and R1 do). Then distill this amplified model into a new model, $M_1$, that is better able to produce the final answer from the amplified model without doing the hidden reasoning steps.$^6$ If this works, you now have model that is more capable than GPT-4o without using extra inference-at-deployment. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">By iterating this process of amplification followed by distillation, it may be possible for the LLM (like AlphaGo Zero) to climb a very long way up this ladder before the process runs out of steam. And the time for each iteration may be substantially shorter than the time between major new pre-training runs. Like AlphaGo Zero, the final distilled model could display very advanced capabilities even without amplification. If this all worked, it would be a way of scaling inference-during-training to substantially quicken the rate of AI progress.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">It is not at all clear whether this would work. It may plateau quickly, or require rapidly growing parameter counts to distill each new model, or take too long per step, or too many steps, or require years worth of engineering effort to overcome the inevitable obstacles that arise during the process.$^7$ But AlphaGo Zero does give us a proof of concept of a small team at a leading lab achieving take-off with such a process and riding its rapid ascent to reach capabilities far beyond the former state-of-the-art.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">So iterated distillation and amplification provides a plausible pathway for scaling inference-during-training to rapidly create much more powerful AI systems. Arguably this would constitute a form of ‘recursive self-improvement’ where AI systems are applied to the task of improving their own capabilities, leading to a rapid escalation. While there have been earlier examples of this, they have often been on narrow domains (e.g. the game of Go) or have only applied to certain cognitive abilities (e.g. ‘learning how to learn’) and so been bottlenecked on other abilities. Iterated distillation and amplification of LLMs is a version that could credibly learn to improve its own general intelligence.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">What does this mean for AI governance? A key implication is that scaling inference-during-training may mean we have less transparency into the best current models. While this use of inference inside the training process would reach the EU AI Act’s compute threshold, that threshold only requires oversight when the model is deployed.$^8$ Thus it may be possible for companies to substantially scale up the intelligence of their leading models without anyone outside knowing. AI governance may then have to proceed from a state of greater uncertainty about the state of the art. Relatedly, the lack of transparency would mean the public and policymakers wouldn’t be able to try these state-of-the-art models and so the overton window of available policy responses won’t be able to shift in response to them. This would lead to less regulation and a more abrupt shock to the world when the models at the top of the training ladder are deployed.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">But perhaps most importantly, the possibility of training general models via iterated distillation and amplification could shorten the timelines until AGI systems with transformative impacts on the world. If this was combined with a lack of transparency about state of the art models during internal scaling, we couldn’t know for sure if timelines were shortening or not, making it hard to know whether emergency measures were required. All of this suggests that policies to require disclosure of current capabilities (and immediate plans for greater capabilities) would be very valuable.</p><h2 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Conclusions</strong></h2><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The shift from scaling pre-training compute to scaling inference compute may have substantial implications for AI governance.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">On the one hand, if much of the remaining scaling comes from scaling inference at deployment, this could have implications including:</p><ul data-rte-list="default"><li><p class="" style="white-space:pre-wrap;">Reducing the number of simultaneously served copies of each new model</p></li><li><p class="" style="white-space:pre-wrap;">Increasing the price of first human-level AGI systems</p></li><li><p class="" style="white-space:pre-wrap;">Reducing the value of securing model weights</p></li><li><p class="" style="white-space:pre-wrap;">Reducing the benefits and risks of open-weight models</p></li><li><p class="" style="white-space:pre-wrap;">Unequal performance for different tasks and for different users</p></li><li><p class="" style="white-space:pre-wrap;">Changing the business model and industry structure</p></li><li><p class="" style="white-space:pre-wrap;">Reducing the need for monolithic data centres</p></li><li><p class="" style="white-space:pre-wrap;">Breaking the strategy of AI governance via compute thresholds</p></li></ul><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">On the other hand, if companies instead focus on scaling up the inference-during-training, then they may be able to use reasoning systems to create the high-quality training data needed to allow pre-training to continue. Or they may even be able to iterate this in the manner of AlphaGo Zero and scale faster than ever before — up the ladder of iterated distillation and amplification. This possibility may lead to:</p><ul data-rte-list="default"><li><p class="" style="white-space:pre-wrap;">Less transparency into the state of the art models</p></li><li><p class="" style="white-space:pre-wrap;">Less preparedness among the public and policymakers </p></li><li><p class="" style="white-space:pre-wrap;">Shorter timelines to transformative AGI</p></li></ul><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Either way, the shift to inference-scaling also makes the future of AI less predictable than it was during the era of pre-training scaling. Now there is more uncertainty about how quickly capabilities will improve and which longstanding features of the frontier AI landscape will still be there in the new era. This uncertainty will make planning for the next few years more difficult for the frontier labs, for their investors, and for policymakers. And it may provide a premium on agility: on the ability to first spot what is happening and pivot in response. </p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">All of this analysis should be taken just as a starting point for the effects of inference-scaling on AI governance. As this transition continues it will be important for the field to track the which types of inference-scaling are happening and thus better understand which of these issues we are facing.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">&nbsp;</p><h2 style="text-align:justify;white-space: normal !important;white-space:pre-wrap;"><strong>Appendix. Comparing the costs of scaling pre-training vs inference-at-deployment</strong></h2><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Scaling up pre-training by an order of magnitude and scaling up inference-at-deployment by an order of magnitude may have similar effects on the capabilities of a model, but they can have quite different effects on the total compute cost of the project. Which one is more expensive depends on the circumstances in a rather complex way.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Let’s focus on the total amount of compute used for an AI system over its lifetime as the cost of that system (though this is not the only thing one might care about). The total amount of compute used for an AI system is equal to the amount used in training plus the amount used in deployment:</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$C = C_{pre-training} + C_{post-training} + C_{deployment}$</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Let $N$ be the number of parameters in the model, $D$ be the number of data tokens it is trained on, $d$ be the number of times the model is deployed (e.g. the number of questions it is asked) and $I$ be the number of inference steps each time it is deployed (e.g. the number of tokens per answer). Then this approximately works out to:$^9$</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$C&nbsp; \approx &nbsp;ND&nbsp; +&nbsp; C_{post-training} &nbsp;+&nbsp; dNI$</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">Note that scaling up the number of parameters, $N$, increases both pre-training compute and inference compute, because you need to use those parameters each time you run a forward pass in your model. But scaling up $D$ doesn’t directly affect deployment costs. Some typical rough numbers for these variables in GPT-4 level LLMs are:</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class=""><em>N </em>= 10$^{12}$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<em>D</em> = 10$^{13}$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<em>I</em> = 10$^{3}$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<em>d</em> = ?</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">On this rough arithmetic, the deployment costs overtake the pre-training costs when the total number of tokens generated in deployment ($dI$) is greater than the total number of training tokens $D$. That would require $d$ &gt; 10$^{10}$. Apparently, this is usually the case, with deployment compute exceeding total training compute on commercial frontier systems.$^{10}$</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">The most standard way of training LLMs to minimise training compute involves scaling up $N$ and $D$ by the same factor. For example, if you scale up training compute by 1 OOM, that means 0.5 OOMs more parameters and 0.5 OOMs more data. So scaling up training compute by 1 OOM also increases deployment compute by 0.5 OOM. In contrast, scaling up inference-at-deployment by an order of magnitude doesn’t (directly) affect pre-training compute.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">When either the pre-training compute ($ND$) or the deployment compute ($dNI$) is the bulk of the total (including $C_{post-training}$), there are some simple approximations for the costs of scaling. If $C_{pre-training} \gg C_{post-training} + C_{deployment}$, then scaling pre-training by 10x increases costs by nearly 10x, while scaling inference-at-deployment ($I$) by 10x doesn’t affect the total much. Whereas if $C_{deployment} \gg C_{pre-training} + C_{post-training}$, then scaling pre-training by 10x increases costs by ~3x (from the larger number of parameters needed at deployment), while scaling inference-at-deployment by 10x increases costs by nearly 10x. So there is some incentive to balance these numbers where possible.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">It is important to note that the costs of scaling inference-at-deployment depend heavily on how much deployment you are doing. If you just use the model to answer a single question, then you could scale it all the way until it generates as many tokens as you pretrained on (i.e. trillions) before it appreciably affects your overall compute budget. While if you are scaling up the inference used for every question, your overall compute budget could be affected even by a 2x scale up. <br><br></p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^1$ e.g. Dario Amodei has said ‘Every once in a while, the underlying thing that is being scaled changes a bit, or a new type of scaling is added to the training process. From 2020-2023, the main thing being scaled was&nbsp;<em>pretrained models</em>: models trained on increasing amounts of internet text with a tiny bit of other training on top. In 2024, the idea of using&nbsp;<em>reinforcement learning</em>&nbsp;(RL) to train models to generate chains of thought has become a new focus of scaling.’</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^2$ Or, somewhat equivalently, it might be better thought of as slowing these systems down by that factor (e.g. 100x). Amodei’s estimate is that AI systems are currently 10x–100x human speed, but if they reach intelligence via inference scaling, they may be slower than humans. Both ways of looking at it lead to the same reduction in the ‘human-days-equivalent of AI work each day’ when the systems are switched from training to deployment.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^3$ For AlphaGo Zero, the goal was to start with zero information about Go and learn everything, so $M_0$ was simply a randomly initialised network. But it is also possible to start with a more advanced network as $M_0$, such as one trained to imitate human behaviour.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^4$ Given 29 million games of self-play and a set-up with 25,000 games before each distillation, there were presumably 1,160 iterations of amplification and distillation before it reached its plateau, such that $M_{max}$ is $M_{1160}$.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^5$ Like o1 and R1, we would presumably include additional RL post-training to prepare it for use in inference scaling.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^6$ Here $M_1$ could be a fresh model distilled from the inference-scaled $M_0$, or it could be $M_0$ with fine-tuning to make it behave more like the inference-scaled $M_0$.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^7$ It is also possible that it will work in some domains (such as mathematics and coding) but not others, leading to superhuman capabilities in several new domains, but not across the board.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^8$ And only when deployed inside the EU itself, where OpenAI’s inference-scaled model deep research is conspicuously absent.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^9$ I’m simplifying some of the details (such as the precise coefficients for each term) to make the overall structure of the equation clearer.</p><p style="text-align:justify;white-space: normal !important;white-space:pre-wrap;" class="">$^{10}$ This has led to methods of training that use more training compute than is Chinchilla-optimal because the smaller model leads to compensating savings on the deployment compute</p>
</div>




















  
  



</div></div><div class="sqs-block website-component-block sqs-block-website-component sqs-block-code code-block" data-block-css="[&quot;https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.code/7fe3dcc7-477a-4b2e-b03c-bf731aeb3444_29/website.components.code.styles.css&quot;]" data-block-scripts="[&quot;https://definitions.sqspcdn.com/website-component-definition/static-assets/website.components.code/7fe3dcc7-477a-4b2e-b03c-bf731aeb3444_29/website.components.code.visitor.js&quot;]" data-block-type="1337" data-definition-name="website.components.code" data-sqsp-block="code" id="block-yui_3_17_2_1_1750675232072_153864"><div class="sqs-block-content"><div
  class="sqs-code-container"
  
  
    data-localized="{&quot;enableSafeModeTitle&quot;:&quot;Embedded Scripts&quot;,&quot;exitSafeModeButton&quot;:&quot;Exit safe preview&quot;,&quot;enableSafeModeText&quot;:&quot;This block contains embedded scripts. Embedded scripts are disabled while you're logged in and editing your site.&quot;,&quot;enableSafeModeButton&quot;:&quot;Preview in safe mode&quot;,&quot;exitSafeModeTitle&quot;:&quot;Safe Preview&quot;,&quot;exitSafeModeText&quot;:&quot;Please view the page after logging out for accurate rendering.&quot;,&quot;globalSafeMode&quot;:&quot;Embedded Code: This block contains embedded code that has been disabled.&quot;,&quot;scriptDisabled&quot;:&quot;Script Disabled&quot;}"
  
  
>
  
    <p class="date">12 February 2025</p>
  
</div>
</div></div></div></div></div>

    <!--BLOG INJECTION-->

    <!-- MathJax: Reset equation counter after every blog entry -->
$\setCounter{0}$    

    <!--CATEGORIES-->

    <div class="post-meta">

      <!--COMMENTS, SHARE, LIKE-->
      <div class="comment-share-like">
        
        
        <a class="like" title="Like this">
  <span class="sqs-simple-like" data-item-id="67ac8d877fb3420883967ac0" data-like-count="0">
    <span class="like-icon"></span>
    <span class="like-count"></span>
  </span>
</a>
        <span class="squarespace-social-buttons inline-style" data-system-data-id="" data-asset-url="https://static1.squarespace.com/static/562652dbe4b05bbfdc596fd7/605a132f7a36283cfa0c4af9/67ac8d877fb3420883967ac0/1750675922012/" data-record-type="1" data-full-url="/writing/inference-scaling-reshapes-ai-governance" data-title="Inference Scaling Reshapes AI Governance"></span>
      </div>
      
    </div>

  </article>
</section>




  <div class="pagination">
    
    <div class="left">
      <span class="prev"><a href="/writing/half-life">&larr;</a></span>
      <span class="prev-title"><a href="/writing/half-life"><strong>May 07, 2025</strong><br>Is there a Half-Life for the Success Rates of AI Agents?</a></span>
    </div>
    
    
    <div class="right">
      <span class="next"><a href="/writing/inference-scaling-and-the-log-x-chart">&rarr;</a></span>
      <span class="next-title"><a href="/writing/inference-scaling-and-the-log-x-chart"><strong>January 21, 2025</strong><br>Inference Scaling and the Log-x Chart</a></span>
    </div>
    
  </div>

</div> <!-- /.post-wrapper -->


  <!-- COMMENTS -->
            </div>
        </div>
        </section>
      </div> <!-- end .wrapper -->

      

      <footer id="footer" class="cf">
        <div class="sqs-layout sqs-grid-1 columns-1 empty" data-layout-label="Left Footer Content" data-type="block-field" data-updated-on="1445527703966" id="footerBlockLeft"><div class="row sqs-row"><div class="col sqs-col-1 span-1"></div></div></div>
        <div class="sqs-layout sqs-grid-1 columns-1 empty" data-layout-label="Right Footer Content" data-type="block-field" data-updated-on="1445357849024" id="footerBlockRight"><div class="row sqs-row"><div class="col sqs-col-1 span-1"></div></div></div>
      </footer>
    </div> <!-- end .outer-wrapper -->

    <!--INJECTION POINT FOR TRACKING SCRIPTS AND USER CONTENT FROM THE CODE INJECTION TAB-->

    <!-- MathJax: Re-typset page in case it is dynamic, per https://stackoverflow.com/questions/25839396/is-it-possible-to-use-mathjax-on-squarespace -->
<script> 
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script><script data-sqs-type="imageloader-bootstrapper">if(window.ImageLoader) window.ImageLoader.bootstrap({}, document);</script><script>Squarespace.afterBodyLoad(Y);</script><svg xmlns="http://www.w3.org/2000/svg" version="1.1" style="display:none" data-usage="social-icons-svg"><symbol id="email-icon" viewBox="0 0 64 64"><path d="M17,22v20h30V22H17z M41.1,25L32,32.1L22.9,25H41.1z M20,39V26.6l12,9.3l12-9.3V39H20z"/></symbol><symbol id="email-mask" viewBox="0 0 64 64"><path d="M41.1,25H22.9l9.1,7.1L41.1,25z M44,26.6l-12,9.3l-12-9.3V39h24V26.6z M0,0v64h64V0H0z M47,42H17V22h30V42z"/></symbol></svg>


  </body>
</html>
