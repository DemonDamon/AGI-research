# RLHF Book by Nathan Lambert

**来源**: [https://rlhfbook.com/](https://rlhfbook.com/)

---

# 
    RLHF Book by Nathan Lambert
  

原文链接: https://rlhfbook.com/

# Reinforcement Learning from Human Feedback

A short introduction to RLHF and post-training
focused on language models.

Nathan Lambert

## Abstract

Reinforcement learning from human feedback (RLHF) has become an
important technical and storytelling tool to deploy the latest machine
learning systems. In this book, we hope to give a gentle introduction
to the core methods for people with some level of quantitative
background. The book starts with the origins of RLHF – both in recent
literature and in a convergence of disparate fields of science in
economics, philosophy, and optimal control. We then set the stage with
definitions, problem formulation, data collection, and other common
math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction
tuning to training a reward model and finally all of rejection
sampling, reinforcement learning, and direct alignment algorithms. The
book concludes with advanced topics – understudied research questions
in synthetic data and evaluation – and open questions for the
field.

## Changelog

**2 July 2025** : Add tool use chapter (see [PR](https://github.com/natolambert/rlhf-book/pull/122))

**6 June 2025** : v1.1. Lots of RLVR/reasoning improvements (see [PR](https://github.com/natolambert/rlhf-book/pull/120))

**14 Apr. - 16 Apr. 2025** : Finish v0. Overoptimization, open questions, etc.

**6 Apr. - 12 Apr. 2025.**: Evaluation section

**28 Mar. - 5 Apr. 2025.**: Research on RLHF x Product, cleaning, improving website, reasoning section

**17 Mar. - 27 Mar 2025.**: Improving policy gradient section, minor changes

**6 Mar. - 16 Mar 2025.**: Finish DPO, major cleaning

**26 Feb. - 5 Mar 2025.**: Start DPO chapter, improve intro

**20-25 Feb. 2025**: Improve SEO, add IFT chapter, minor edits

**10-15 Feb. 2025**: RM additions, preference data, cleaning, policy gradient finalization

**8 Feb. 2025**: RM additions, editing, cleaning

**4 Feb. 2025**: PPO and GAE

**2 Feb. 2025**: Added changelog, revamped introduction,

## Acknowledgements

I would like to thank the following people who helped me directly with this project: Costa Huang, (and of course Claude). Indirect shout-outs go to Ross Taylor, Hamish Ivison, John Schulman, Valentina Pyatkin, Daniel Han, Shane Gu, Joanne Jang, LJ Miranda, and others in my RL sphere.

Additionally, thank you to the [contributors on GitHub](https://github.com/natolambert/rlhf-book/graphs/contributors) who helped improve this project.

---

*爬取时间: 2025-11-28 21:57:47*
