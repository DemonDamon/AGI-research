# Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback

**类型**: 论文 (Paper)

**链接**: [https://link.springer.com/article/10.1007/s10676-025-09837-2](https://link.springer.com/article/10.1007/s10676-025-09837-2)

## 摘要

A critical evaluation of RLHF's limitations in capturing the complexities of human ethics and achieving AI safety, focusing on the sociotechnical aspects of the HHH principle.

## 下载

请访问上述链接查看完整论文。
