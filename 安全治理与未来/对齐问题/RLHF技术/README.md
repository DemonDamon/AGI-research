# RLHFæŠ€æœ¯

> ğŸ“ ä½ç½®ï¼šå®‰å…¨æ²»ç†ä¸æœªæ¥ > å¯¹é½é—®é¢˜ > RLHFæŠ€æœ¯

## ğŸ“– ç®€ä»‹

åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning from Human Feedbackï¼‰ã€‚


## ğŸ“š èµ„æ–™åˆ—è¡¨

æœ¬ç›®å½•æ”¶é›†äº† 4 ä»½ç›¸å…³èµ„æ–™ï¼š

- [Deep reinforcement learning from human preferences](./01_Deep-reinforcement-learning-from-human-preferences.md)
- [Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback](./02_Helpful-harmless-honest-Sociotechnical-limits-of-AI-alignment-and-safety-through-Reinforcement-Learn.md)
- [RLHF 101: A Technical Tutorial on Reinforcement Learning from Human Feedback](./03_RLHF-101-A-Technical-Tutorial-on-Reinforcement-Learning-from-Human-Feedback.md)
- [RLHF Book by Nathan Lambert](./04_RLHF-Book-by-Nathan-Lambert.md)


## ğŸ“š èµ„æ–™åˆ—è¡¨

æœ¬ç›®å½•æ”¶é›†äº† 4 ä»½ç›¸å…³èµ„æ–™ï¼š

- [Deep reinforcement learning from human preferences](./01_Deep-reinforcement-learning-from-human-preferences.md)
- [Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback](./02_Helpful-harmless-honest-Sociotechnical-limits-of-AI-alignment-and-safety-through-Reinforcement-Learn.md)
- [RLHF 101: A Technical Tutorial on Reinforcement Learning from Human Feedback](./03_RLHF-101-A-Technical-Tutorial-on-Reinforcement-Learning-from-Human-Feedback.md)
- [RLHF Book by Nathan Lambert](./04_RLHF-Book-by-Nathan-Lambert.md)

## ğŸ–¼ï¸ ç›¸å…³å›¾ç‰‡

æœ¬ç« èŠ‚çš„ç›¸å…³å›¾ç‰‡èµ„æºå­˜æ”¾åœ¨ `images/` ç›®å½•ä¸‹ã€‚

---

[â¬†ï¸ è¿”å›ä¸Šçº§](../README.md) | [ğŸ  è¿”å›é¦–é¡µ](../../../README.md)
